\documentclass{article}
% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}
% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}
\usepackage[final]{nips_2016}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xspace}
\usepackage{amssymb,amsmath,amsthm,graphicx,url,booktabs,microtype}
% ---------------- todocmd ----------------- %
\usepackage[]{todonotes} % insert [disable] to disable all notes.
\newcommand{\Todo}[1]{\todo[author=Pushpendre,size=\small,inline]{#1}}
% --------- refcmd ------------ %
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\thref}[1]{Theorem~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\charef}[1]{Chapter~\ref{#1}}
\newcommand{\algref}[1]{Algorithm~\ref{#1}}
\renewcommand{\eqref}[1]{Equation~\ref{#1}}
% ------------ watermark -------------- %
\usepackage{draftwatermark}
\SetWatermarkText{Confidential}
\SetWatermarkScale{.3}
% ------------ Paper Specific Math -------- %
\newcommand{\Vo}{V^{\textrm{out}}}
\newcommand{\Vp}[1]{V^{[.#1]}}
\newcommand{\vo}[1]{v^{\textrm{out}}_{#1}}
\newcommand{\ip}[2]{\left\langle#1, #2\right\rangle}
\newcommand{\wv}{Word2Vec\xspace}
\title{Cocoon: The Correct Way to Create Compositional $N$-Gram Embeddings}
\author{Pushpendre}
\begin{document}
\maketitle
\begin{abstract}
We all know how to embed tokens using \wv. But how can we compute
embeddings of bigrams or trigrams once we have computed the embeddings?
One way will be to retrain the whole model but that is so slow and uncool.

\end{abstract}
\section{Introduction}
\label{sec:introduction}
Word embeddings have provided a novel and effective way of injecting background
knowledge into supervised ML taks through an efficient and completely
unsupervised process.

Although \wv provides a good way to induce representations of single
tokens, however the process for creating bigram embeddings is far from
satisfactory. The first paper that popularized this method used a heuristic of
converting bigrams to tokens before learning the embeddings. However this method
will require the knowledge of all $n$-grams that need to be embedded which is
often not known before hand. Assume that you are creating a naive bayes
classifier and want to use the embeddings of unigrams and bigrams as features as
done in~\cite{wang2012baselines}. In order to create represent bigrams we
will have to know before hand which bigrams need to be embedded and then we will
have to run the entire process of \wv again which is inefficient.

Therefore we believe that a method for constant time on the fly computation of
compositional embeddings will be an important contribution because it is a efficient
way of injecting background knowledge about phrases into supervised ML tasks.

In the rest of the paper we present an intuitive method for creating compositional vector
representations of $n$-grams.

\section{Method}
\label{sec:method}
\paragraph{Notation} Let $s$ be a sequence of words and let $s_j$ be the $j$th word of sequence $s$.
Let $|s|$ be the length of the sequence and let $S$ be the set of all sequences.
% Let us use the symbol $j$ to index this set i.e. $s_j \in S$.
Let $W$ denote an indexed set of words and
let $w$ denote a generic word and $w_i$ denote the $i$th word of $W$.
Let $V$ and $\Vo$ denote indexed sets of vectors corresponding to $W$,
i.e. $v_i \in V$ and $\vo{i} \in \Vo$ correspond to $w_i \in W$. The notation
$[., .)$ denotes a set of integers that containing successive integers starting
from and including the left  and going up to the right hand argument.
These two sets
of vectors correspond to the input and output representations of a word as
described in~\cite{mikolov2013distributed}.

\paragraph{Background} The \wv model maximizes the following average log-probability of the
sentence averaged over the entire corpus.
\begin{equation}
  \label{eq:w2vec1}
  \frac{1}{|S|} \sum_{s \in S} \frac{1}{|s|} \sum_{j} \sum_{k \in [j-c,0) \cup (0,j+c]} \log p(s_k | s_j)
\end{equation}
\noindent The probability of token $s_k$ given token $s_j$ is computed as the
softmax over the inner products of the embeddings of the two tokens. I.e.
\begin{equation}
  \label{eq:w2vec2}
  p(s_k | s_j) = \frac{\exp\left(\ip{\vo{s_k}}{v_{s_j}}\right)}{\exp\left(\sum_{w \in W}\ip{\vo{w}}{v_{s_j}}\right)}
\end{equation}

\subsection{Our Method}
\label{ssec:our-method}
\begin{figure}[htbp]
  \centering
  \begin{tabular}{c}
      The \framebox{squeaky \framebox{wheel}} gets replaced .
  \end{tabular}
  \caption{Example}
  \label{fig:pretty-example}
\end{figure}
The \wv model encodes a word $w$ using a single embedding $v_w$ that must
maximize the log probability of the tokens that occur around it. This encourages
the embedding of a word to be representative of the context surrounding
it. However a careful look reveals that the context around a word can be split
into multiple categories. For example each word has atleast two types of
contexts, a left context and a right context.
For example \figref{fig:pretty-example} which shows a unigram token \textit{wheel}
and a bi-gram \textit{squeaky wheel} which envelopes this unigram. Intuitively the
right context of the bigram \textit{squeaky wheel} should be the same as the
right context of the unigram \textit{wheel} and therefore the representations
of the vector representation of the right context of \textit{wheel} should be
similar to the embedding of the right context of \textit{squeaky
  wheel}. Similarly the embedding of the left context of \textit{squeaky wheel} should be
similar to the left embedding of \textit{squeaky}.

This observation suggests a way of creating an embedding for bigrams. We can
parameterize each word $w$ with \textit{two} embeddings $v_w^0$ and $v_W^1$.
$v_w^0$ encodes the left context and $v_W^1$ encodes the right context.
This strategy can be implemented by replacing the probability distribution in
\eqref{eq:w2vec2} as follows:
\begin{equation}\label{eq:w2vec3}
  p(s_k | s_j) = \frac{\exp\left(\ip{\vo{s_k}}{v_{s_j}^{[k>j]}}\right)}{\exp\left(\sum_{w \in W}\ip{\vo{w}}{v_{s_j}^{[k<j]}}\right)}
\end{equation}

The above strategy gives us a simple and efficient way of creating embeddings
for $0$-grams(i.e. unigrams) and $1$-grams(i.e. bigrams). The representation of
a single word $w$ or unigram is the concatenation of $[v_w^0; v_w^1]$ and the
represenation of a bigram $u,w$ is the concatenation of $v_u^0; v_w^1$.

Although the strategy presented above is simple, efficient and fast however it
is undesirable for trigrams and longer $n$-grams because it ignores the words in
the middle. Even for encoding bigrams this strategy is undesirable because
both the left and right contexts of a bigram are affected by both the words in the
bigrams. \figref{pretty-example-2} gives an example.
We now present our strategy to eliminate these shortcomings. Let us reconsider
the example in \figref{fig:pretty-example}. Intuitively the right context of
\textit{squeaky wheel} should be a combination of the right context
\textit{wheel} and the one word separated right context of \textit{squeaky}.
In general the $m$ word separated right context of a word $w$ is defined to be
the bag of words that appear to the right of $w$ separated by $m$ tokens or more
from $w$. Analogously the $m$ word separated left context of a word is defined
as all the words that appear at least $m$ tokens before $w$. In general we will
call these the ``{separated contexts}'' when the value of $m$ and the
distinction between left and right does not matter. Our strategy
springs from the realization that every separated context for a word can be
encoded with a separate word vector.\footnote{Preferably initialized from the
  same value.} The original \wv formulation uses the same word vector for each
separated context and the formulation above uses the $0$-left context and the
$0$-right contexts of a word. However the original \wv objective in
\eqref{eq:w2vec1} can be generalized as follows:
\begin{align}\label{eq:general}
  &\frac{1}{|S|} \sum_{s \in S} \frac{1}{|s|} \sum_{j} \sum_{m \in [0, M]} \sum_{k \in [j-c,m) \cup (m,j+c]} \log p_m(s_k | s_j)\\
  &p_m(s_k | s_j) = \frac{\exp\left(\ip{\vo{s_k}}{v_{s_j}^{(m,[k>j])}}\right)}{\exp\left(\sum_{w \in W}\ip{\vo{w}}{v_{s_j}^{(m,[k<j])}}\right)}
\end{align}
Optimizing this procedure results in $2M+2$ word vectors. These vectors can be
averaged to compute the embeddings of an $n$-gram by averaging the suitable
vectors. Let us return to our example. In order to create the left embedding of the
bigram \textit{squeaky wheel} we will average the embedding
$v_{\textrm{squeaky}}^{(0,0)}$ and $v_{\textrm{wheel}}^{(1,0)}$ and the right embedding of the
bigram can be computed as the average of
$v_{\textrm{squeaky}}^{(1,1)}$ and $v_{\textrm{wheel}}^{(0,1)}$.

\section{Experiments and Results}
\label{sec:experiments-results}
Our model decomposes the embedding of a word into two components that are
separately responsible for encoding the left and the right contexts. To
establish the usefulness of our model we perform the following experiments.

Train the model with $M=0, 1$ and $2$. For each setting of $M$ we test
whether the embeddings are performant as embeddings for words themselves.
We check the performance of $[v_w^{(m,0)}; v_w^{(m,1)}]$ as embeddings of
the word $w$ on the word similarity and word analogy tasks and compare them to
standard \wv.

\begin{table}[htbp]
  \centering
  \begin{tabular}{l c c c c c}
    Method       & WN-353 & MEN & SimLex & Analogy \\
    MVLSA        &                        \\
    Glove        &                        \\
    Word2Vec     &                        \\
    \hline
    Cocoon (M=0) &                        \\
    Cocoon (M=1) &                        \\
    Cocoon (M=2) &                        \\
  \end{tabular}
  \caption{Performance of word embeddings on Standard Word Similarity Tasks}
  \label{tab:wsim}
\end{table}


Prior work has often aproximated the embedding of an $n$-gram by averaging the
embeddings of its constituent words, thereby an $n$-gram as a bag of words.
 our next experiment

The most important application of our work is as a feature representation on
several classification benchmarks that are commonly used for evaluating sentence
representations. For example~\cite{wang2012baselines} showed that using bigram
features can be very useful for predicting moview review sentiment and other
tasks. \Todo{We show in \tabref{tab:classification} that our features outperform the
\textit{skip-thought} and \textit{paragraph-vectors} on the following 5 tasks
when combined with the NB-SVM features.
Movie Review Sentiment (MR), Customer Product Reviews (CR),
Subjectivity/Objectivity Classification, Opinion Polarity (MPQA) and
Question Type Classifcation}

\begin{table}[htbp]
  \centering
  \begin{tabular}{l c c c c c}
    Method            & MR & CR & SUBJ & MPQA & TREC \\
    NB-SVM            &    &    &      &      &      \\
    MNB               &    &    &      &      &      \\
    Paragraph Vector  &    &    &      &      &      \\
    bow               &    &    &      &      &      \\
    Combine Skip      &    &    &      &      &      \\
    Combine Skip + NB &    &    &      &      &      \\\hline
    Cocoon            &    &    &      &      &      \\
    Cocoon + NB       &    &    &      &      &      \\
  \end{tabular}
  \caption{Classification accuracies on short text classification benchmarks
    that use $n$-gram representations.}
  \label{tab:classification}
\end{table}

\section{Discussion}
\label{sec:discussion}

\bibliographystyle{abbrv}
\bibliography{reference}
\end{document}
