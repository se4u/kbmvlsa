\documentclass[paper=a4,fontsize=11pt]{scrartcl}
\usepackage{underscore,changepage,booktabs,enumitem}
\usepackage[round]{natbib}
\usepackage[dvipsnames]{xcolor}
\usepackage[normalem]{ulem}
\usepackage{framed}
\definecolor{shadecolor}{rgb}{1.0,0.8,0.3}
\usepackage[T1]{fontenc}
\usepackage[english]{babel} % English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{amsmath,amsfonts,amsthm,url,xspace,amssymb,mathrsfs}
\usepackage[pdftex]{graphicx}

% http://tex.stackexchange.com/questions/128033
% undefining-leftbar-environment-to-eliminate-multiple-definitions
% Work by adding these two lines.
\let\leftbar\undefined
\let\endleftbar\undefined

\usepackage{thmtools}
\declaretheorem[%
  style=plain,%
  thmbox={style=S,bodystyle=\normalfont},%
  name=Note,%
  within=section,%
  shaded={rulecolor=Lavender,rulewidth=2pt},%
]{thmnote}
\declaretheorem[%
  style=plain,%
  thmbox={style=L,bodystyle=\normalfont},%
  name=Example,%
  within=section,%
  shaded={bgcolor=gray!10},%
]{example}
%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering\normalfont\scshape}
%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\usepackage[disable]{todonotes} % or [disable] notes.
\pagestyle{fancyplain}
\fancyhead{}                        % No page header
\fancyfoot[L]{}                     % Empty
\fancyfoot[C]{}                     % Empty
\fancyfoot[R]{\thepage}             % Pagenumbering
\renewcommand{\headrulewidth}{0pt}  % Remove header underlines
\renewcommand{\footrulewidth}{0pt}  % Remove footer underlines
\setlength{\headheight}{13.6pt}
%%% Define a custome column type.
\usepackage{dcolumn}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\newcommand{\eg}{e.g.,\xspace}
\newcommand{\Eg}{E.g.,\xspace}
\newcommand{\etal}{\textit{et~al.\xspace}}
\newcommand{\etc}{etc.\@\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\Ie}{I.e.,\xspace}
\newcommand{\secref}[1]{section~\ref{#1}}
\newcommand{\Secref}[1]{Section~\ref{#1}}
\newcommand{\tabref}[1]{table~\ref{#1}}
\newcommand{\Tabref}[1]{Table~\ref{#1}}
\newcommand{\exref}[1]{example~\ref{#1}}
\newcommand{\Exref}[1]{Example~\ref{#1}}
\newcommand{\figref}[1]{figure~\ref{#1}}
\newcommand{\Figref}[1]{Figure~\ref{#1}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname*{argmax}}}
\newcommand{\note}[1]{\todo[author=PR,color=blue!40,size=\small,fancyline,inline]{Note: #1}}
\newcommand{\Todo}[1]{\todo[author=PR,size=\small,inline]{Todo: #1}}
\newcommand{\FW}[1]{\todo[author=PR,color=pink!20,size=\small,inline]{Future Work: #1}}


%%% Equation and float numbering
\numberwithin{equation}{section}    % Equationnumbering: section.eq#
\numberwithin{figure}{section}      % Figurenumbering: section.fig#
\numberwithin{table}{section}       % Tablenumbering: section.tab#
%%% Institution and Authors
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}     % Horizontal rule
% \title{
%   % \vspace{-1in}
%   \usefont{OT1}{bch}{b}{n}
%   \normalfont\normalsize\textsc{HLTCOE, Johns Hopkins University}\\[25pt]
%   \horrule{0.5pt}\\[0.4cm]
%   \huge Entity recommendations on a Cold Start Knowledge Graph\\
%   \horrule{2pt}\\[0.5cm]
% }
% \author{
%   \normalfont\normalsize
%   Pushpendre Rastogi\\
%   %[-3pt]\normalsize\today%%Optional Date
% }
\title{
  \vspace{-1in}
  %\usefont{OT1}{bch}{b}{n}
  {\normalfont\huge Entity recommendations on a Cold Start Knowledge Graph}\\
  {\normalfont\normalsize\textsc{HLTCOE, Johns Hopkins University}}\\
}
\author{{\normalfont\normalsize Pushpendre Rastogi}
  \and
  {\normalfont\normalsize Vince Lyzinski}
  \and
  {\normalfont\normalsize Benjamin Van Durme}\\
  %%[-3pt]\normalsize\today%%Optional Date
}

\date{}

\newcommand{\newrel}{41}
\renewcommand{\r}[1]{\textcolor{red}{#1}}
\newcommand{\dataset}[0]{\texttt{BBN2} dataset\ }
\newcommand{\ontology}{\textsc{Adept} Ontology\ }
\newcommand{\task}{CS-KBP 2015 task\ }
\newcommand{\dg}{$^\dagger$}
\newcommand{\ddg}{$^\ddagger$}
\newcommand{\OPTION}[0]{\textcolor{red}{OPTION}}
\newcommand{\PK}{P@K\ }

%%% Commands for arcs and states.
\def\overstrike#1#2{{\setbox0\hbox{$#2$}\hbox to \wd0{\hss$#1$\hss}\kern-\wd0\box0}}
\newcommand{\bigstaten}[1]{\overstrike{\protect\raisebox{1pt}{\mbox{\scriptsize #1}}}{\mbox{\Large $\bigcirc$}}}
\newcommand{\shortarc}[1]{\protect\raisebox{-1pt}{$\stackrel{#1}{\longrightarrow}$}}
\newcommand\given{\:\vert\:}
\newcommand{\remove}[1]{}
% --------------------------------------------------- %
% Globally Override the special status of # character %
% --------------------------------------------------- %
\catcode`#=12
\begin{document}
\maketitle
\section{Introduction}
\label{sec:introduction}
In this report we consider the Adept Knowledge Base (KB), which we
will work with as a graph. This graph is composed of a set of
vertices (\ie entities, nodes), a set of edges (\ie relations,
predicates, slot-fills) and vertices and edges may have zero or more
properties (\ie attributes, features).

The predominant way in which analysts interact with and make use of knowledge
graphs is by treating knowledge graphs as databases that support evaluation
of queries. An analyst may execute two types of queries on a KB:
\begin{description}[leftmargin=*]
\item[Logical Queries] A analyst may want to retrieve a set of items,
  that lie in a set exactly described via a logical expression. For example a analyst
may ask for all vertices that are connected to the vertex with the highest
degree. A simpler example of a query like this is a static lookup: an
analyst may begin with an entity represented as a vertex connected via a
``name'' attribute to the value ``John Smith'' and then construct a query which
looks for an edge called ``worksFor'', and then ``leaderOf''\footnote{Such
  queries form the evaluation of \task}.
whose value is \texttt{John Smith}\footnote
{Specifically, a knowledge graph may be stored inside a graph database,
an RDF {triplestore} or even inside XML files and then queried using a graph
query language such as Cypher or Gremlin or an RDF query language like SPARQL
or XPath which is the defacto standard for querying XML data.}
\item[Example Queries] We focus on such queries in this work.
  The analyst provides {examples} of items of
  interest/relevance, and wants the system to \textit{nominate} or \textit{recommend}
  items that are similar to the examples. Sometimes the analyst may also
  provide \textit{negative supervision} in the form of examples that are marked
  as uninteresting or irrelevant. Databases that can support such
  example queries and return a sorted list of relevant entities are called
  \textit{Recommendation Systems} or \textit{Recommenders} in short. They may
  also be called \textit{Vertex Nomination Systems}. We call algorithms
that can rank entities in a dataset based on a set of example entities
\textit{Recommendation Algorithms}~(RAs).
\end{description}

Interfaces that enable logical queries view the goal of the
Adept KB as one of aggregating Information Extraction (IE) output from raw
natural language text into a database that can be queried declaratively by an
analyst, similar to the way SQL is used to query a relational database.
The important point is that creating the correct query is left to the analyst
who must craft an expression that succinctly and
efficiently describes the set of interesting or relevant items. Crafting
such a query may be difficult and it may require significant skill on the part of the
analyst (See \Exref{ex:recommender}).
\begin{example}\label{ex:recommender}
  Consider a social network of sportsmen that contains the height
  and weight of people and the sports that they play as well as their friendship
  status. Assume that an analyst needs to retrieve the adult triathlete
  with the lowest lung capacity from this database.

  Clearly our analyst has her job cut out for her, not only will she have to
  figure out the various ways in which people might express that they are
  triathletes, but the attributes of age and lung capacity don't even exist in
  the dataset. Some triathletes may state explicitly that they are
  ``triathlete'' or that their sport of choice is ``triathlon'' or they may
  instead state the three or more sports that they play. Because of this variety
  of expressions individually creating
  the correct filters by hand can be time consuming. On top of that the analyst
  will have to manually figure out some rules for guessing the age and lung capacity of
  a person  based on their height and weight.

  In such a scenario a \textit{Recommender} can be very useful for achieving quick
  results. A general purpose RA based on a probabilistic linear model built with
  third order feature conjunctions can automatically figure out the right
  entities to retrieve from the database and give useful hints to the analyst
  for fast prototyping.
\end{example}
In contrast to logical queries,
example queries do not require the analyst to create a
logical expression by herself. A recommender can use the information stated
in the knowledge graph to infer \textit{new} information that is not
directly stated in
any source document, and recommend relevant items to a user with
minimal supervision. This report lays out definitions and pointers to related areas of
research from various communities, and then presents the performance of
a few canonical VN algorithms (See \Tabref{tab:ra}) when applied to to the Adept
KB via a representative set of example queries.

We conclude from these experiments (See \Secref{sec:conclusions})
that the current Adept KB does not
support example queries well, owing to its severe sparsity
in edges. In short, the ``knowledge graph'' is not much of a graph. The
overwhelming majority of vertices form singleton cliques: entities are
discovered but no relations connect them to the rest of the KB.

We therefore end the report with a proposal to extend the ontology of
the Adept KB, based on conservatively subselecting from an OpenIE
ontology just those relations that best support provided example
queries. We assume in this that DARPA does not wish to allow a
massive (orders of magnitude) increase to the relation types in the
Adept ontology, and thus will pursue a strategy for conservatively
proposing to a KB-engineer a reduced set of relation types such that
if they were included (based on observed performance in their
extraction) then we would have a more sufficient graph for performing
vertex nomination.
\begin{table}[htbp]
  \centering
  \begin{tabular}{l l}
    Method              & Type         \\\hline
    Naive Bayes         & Inductive    \\
    %Block Naive Bayes  & Inductive    \\
    %Binary SVM         & Inductive    \\
    Modified Adsorption & Transductive \\
    Random Walk         & Transductive \\
  \end{tabular}
  \caption{List of Recommendation Algorithms}
  \label{tab:ra}
\end{table}

\section{The \dataset{}}
\label{sec:data}
The TAC Cold Start Knowledge Base Population (CS-KBP) shared task is organized
by NIST to evaluate systems that build a knowledge base (KB) from raw natural
language text~\citep{tac2015cold}. The competing systems extract entities and relations based on
textual clues present in natural language documents and populate a KB according to a shared
schema of entities and relationships. The set of documents is released at the beginning of
the task and the schema is released some time before that.
At the end of the shared task the systems submit their
automatically generated knowledge bases to NIST, which evaluates their accuracy.
BBN participated in the \task where it was the top performing
participant in terms of precision and overall F1 according to the official
results as of November 2015~\citep{bbn2015bonan}. Since BBN's \texttt{BBN2} was one
of its top performing submissions, to ascertain the feasibility of using an
automatically constructed knowledge graph as a recommender, we
performed our experiments on \texttt{BBN2}.

The \dataset is a knowledge graph built according to the \ontology.
It contains three standard entity types: \textsc{PER, ORG, LOC}
that refer to Person, Organization and Location respectively, as well
as four extra types of \textsc{TITLE, DATE, CRIME, URL} (\Tabref{tab:type}).
We will collectively denote entities
of these types as named entities. Technically, we define a named entity to be a
any entity that has only either a canonical string
or a canonical time attached to it and no other attributes.
See~\ref{ex:ne}  for an example.
Because of the errors made during automatic relation extraction there can be multiple
named entities with different identifiers that have approximately the same canonical string
(See \Exref{ex:title-cap}). We call this phenomenon \textit{Denormalization} and we will show
later this phenomenon severely hurts the performance of any recommendation system.
\begin{example}\label{ex:ne}
  The identifier \texttt{9dd1d8d3-9a1f-4ec2-a45b-64cce788eb01} in the dataset has
the type \texttt{Title}
and the canonical string associated with it is \texttt{Almighty King}. This
identifier has no other attributes attached to it.
\end{example}
\begin{example}\label{ex:title-cap}
  The identifiers \texttt{e3ac2b1e-cd3b-4cf8-a1fb-b1f71db123df} and
  \texttt{234f3110-c8ca-49aa-ba7b-a45bb5467c38} have canonical strings \textit{professor} and
  \textit{Professor} respectively which differ only in their capitalization.

  As this example illustrates some of the titles in the \dataset
  lexically differ from each other but mean the same. Matching such fragments
  can help in reducing sparsity and may improve the performance of the vertex
  nomination algorithms. We leave such pre-processing for future work, and
  measure the performance of the VN algorithms on the ADEPT KB as is.
\end{example}
\begin{table}[htbp]
  \centering
  \begin{tabular}{r l l}
    \textbf{Instantiations} & \textbf{Type}      & \textbf{Property} \\
    $125022$                & Person             & canonicalString   \\
    $69544$                 & Organization       & canonicalString   \\
    $23754$                 & GeoPoliticalEntity & canonicalString   \\
    $1761$                  & Title              & canonicalString   \\
    $819$                   & Date               & xsdDate           \\
    $333$                   & Crime              & canonicalString   \\
    $189$                   & URL                & canonicalString   \\\cline{1-1}
    Total $= 221,547$                                                \\
  \end{tabular}
  \caption{The number of instantiations(occurrences) of Named Entities of different types.}
  \label{tab:type}
\end{table}
As \Tabref{tab:type} shows, the \texttt{Person} category is the highest occurring
category with $125$K instantiations
and it is slightly less than double the next largest category of \texttt{Organization}.
The occurrences of the \texttt{URL} type are negligible.

Also we note that the knowledge graph contains a large number of singleton entities.
We can infer this from Tables~\ref{tab:type} and \ref{tab:relation}
since the number of entities is much larger than the total number of edges.

The relations in the \dataset comprise of a small number of relation types.
Just like named entities every instance of a relation has a unique id.
Each instance relates two named entities (See \Exref{ex:rel}).
\begin{example}\label{ex:rel}
  The id \texttt{001ae391-3a55-43d1-8d3e-0557bdd943d1} refers to a relation of the
type \texttt{Resident} which connects its named attribute \texttt{person}
which must have the type \texttt{Person}
with its \texttt{location} attribute which must be of type
\texttt{Geopoliticalentity}.
\end{example}
The total number of regular relations specified in the \task was 41.
The occurrences for different relations follow a power law
(\Tabref{tab:relation}) and the highest occurring \texttt{Role} relation type
that relates a person to his/her title accounts for $30\%$ of the data
out of the total 20 relations that are observed.
The reason for this skew is that the
natural language text itself has a strong bias about the type of relations that are
reported explicitly, and that bias is reflected in the distribution of textual mentions
of relations.
\begin{table}[htbp]
  \resizebox{\textwidth}{!}{
  \begin{tabular}{r l l l}
\textbf{Instances} & \textbf{Relation}       &\textbf{Attribute 1} &\textbf{Attribute 2}\\
42490              & Role                    & person              & role             \\
36631              & EmploymentMembership    & employeeMember      & organization     \\
16288              & Resident                & person              & location         \\
9600               & Leadership              & leader              & affiliatedEntity \\
8705               & Subsidiary\dg           & organization        & subOrganization  \\
7242               & OrgHeadquarter\dg       & organization        & location         \\
2764               & Origin                  & person              & affiliatedEntity \\
1774               & ParentChildRelationship\ddg & parent          & child            \\
1612               & Die                     & person              & place            \\
1317               & SpousalRelationship\ddg & person              & person           \\
1306               & StudentAlum             & studentAlumni       & organization     \\
1281               & Founder                 & founder             & organization     \\
1261               & BeBorn                  & person              & time             \\
530                & InvestorShareholder     & investorShareholder & organization     \\
417                & SiblingRelationship\ddg & person              & person           \\
387                & ChargeIndict            & defendant           & crime            \\
374                & StartOrganization\dg    & organization        & time             \\
369                & Membership\dg           & organization        & member           \\
200                & OrganizationWebsite\dg  & organization        & url              \\
33                 & EndOrganization\dg      & organization        & time             \\\cline{1-1}
Total $= 134,581$ \\
\end{tabular}}
  \caption{Relations Types and their attributes. \dg indicates relations for
    which neither of the arguments are of the type
    \texttt{Person}. \ddg indicates relations for which both of the
    arguments have the type \texttt{Person}.}
  \label{tab:relation}
\end{table}

\subsection{Relations or Attributes}
\label{sec:relat-or-attr}
As shown in \tabref{tab:relation} the \texttt{Role} relation does not relate two people.
It only relates a person and its title.
In this sense, the \texttt{Role} relation act as a feature or an attribute of a person
instead of a relation between them, since the \texttt{Role} for one person can be completely
independent of the Role of another person, unlike the \texttt{Sibling, Spousal} and \texttt{ParentChild}
relationships. The \texttt{Sibling, Spousal} and \texttt{ParentChild} relationships always affects two
people simultaneously in the sense that if a person \textsc{B} is set to be the sibling of \textsc{A} then
\textsc{A} must be set to the sibling of \textsc{B}.
We note that 11 out of 20 relations including the top four most observed relations,
act as attributes of persons instead of relations. So the \textit{person-person} relational
information is lower in comparison to \textit{person-attribute} information.
Also note that only 6 relations relate organization to other organizations or
to locations, so the \textit{attribute-attribute} relational information is
also low. However, in order to leverage the information present in the
\texttt{OrgHeadquarter} relation we use the name of the headquarter of an
organization that is related to a person as an attribute of the person.

\subsection{Sparsity of the Adept KB}
\label{sec:graph-sparsity}
The Adept KB is a graph with more \textsc{PER} entities than edges.
To quantify the sparsity and the connectedness of the Adept KB we present
a table of the degrees of all the \textsc{PER} entities in the Adept KB
in \tabref{tab:deg-hist}. \Tabref{tab:hist-comp} tabulates the
sizes of the components that those \textsc{PER} entities belong to.
Note that both of these tables do not count edges that were not incident
with at least one \textsc{PER} entity with the exception of edges of type
\texttt{OrgHeadquarter}. The edges of type \texttt{OrgHeadquarter} are
included in the counts for the following reasons:
1) The location of the headquarter of the organization that a person is affiliated
with can act as an excellent feature for ranking a person entity.
2) Since the number of locations is small extracting these features does not
cause an explosion in the number of features.
\begin{table}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l| c c c c c c c c c c c c}
    \textbf{Degree}                & 0     & 1     & 2    & 3    & 4    & 5    & 6    & 7   & 8   & 9   & 10  & >10  \\
    \textbf{\textsc{PER} Entities} & 82057 & 16672 & 8997 & 6862 & 3056 & 2521 & 1290 & 846 & 615 & 432 & 329 & 1345 \\
  \end{tabular}}
  \caption{Table of degrees for the \textsc{PER} entities in the Adept KB.}
  \label{tab:deg-hist}
\end{table}

\begin{table}[htbp]
  \centering
  \begin{tabular}{l | c c c c c c c c c }
    \textbf{Component Size}     & 1     & 2  & 3  & 4  & 5 & 42350 \\
  \textbf{Number of Components} & 82057 & 81 & 10 & 10 & 1 & 1     \\
  \end{tabular}
  \caption{The component sizes of the \textsc{PER} entities in the Adept KB.}
  \label{tab:hist-comp}
\end{table}

\Tabref{tab:deg-hist} shows that the number of
completely disconnected \textsc{PER} entities in the Adept KB constitute $65.6\%$
of the total \textsc{PER} entities and that $13.3\%$ of the \textsc{PER}
entities have degree 1 but only $162$ \textsc{PER} entities reside in a ``dumbbell'', \ie a component of size $2$. The majority of the entities that have degree 1 lie in the
giant component that contains $42350$ \textsc{PER} entities. This has a lot of significance since it means that more than a third of the \textsc{PER} entities con be disconnected from the giant component by just removing single edges from the graph. This shows
that the component has a small densely connected core and a large number of leaves.
The above discussion clearly show that the Adept knowledge graph is quite sparse
and weakly connected.


\section{Evaluation}
\label{sec:evaluation}
In order to evaluate RAs we need to simulate the kind of example queries
that an analyst might submit to an RA. We represent a user's query as a
predicate on vertices, where some positive example of the predicate are
provided, and we wish to sort the remaining vertices as to how confident we are
that they satisfy the predicate (are of interest to the user).

We chose to use an existing binarized attribute as a synthetic predicate function that
acts as the oracle that determines whether an entity is relevant or not.
During training and testing we remove this attribute from the data and
only present a few examples of entities with the values of this attribute as labels.
The algorithm then has to predict whether this hidden attribute is $0$ or $1$ for the
rest of the people on the basis of the training data and then to assign a rank to them
such that people with attribute value $1$ rank higher than people with attribute value of $0$.
Two standard evaluation metrics for these tasks are the {Precision@K}(\PK) metric and the
{Area Under Precision Recall Curve}(AUPR) metric and we would report the performance of the systems
on these two metrics and compare them to a random baseline.
\begin{example}\label{ex:eval}
    For example, a user of the recommendation system may mark certain employees of the
  ``White House'' as people of interest and based on these examples the recommendation
  system needs to assign a high rank to the remaining employees of the ``White House'' and
  a low rank to others. Note that the recommendation system does not receive the
  attribute that the person was the employee of the ``White House'' during operation
  and it has to make a guess on the basis of other persons and their attributes.
  Consider a second example, where a predicate function may mark people who are
  ``authors'' as people of interest and request the recommendation system to find more
  people who are ``authors''. \Tabref{tab:pred-func} lists the 6 relations types that we
  use to create a 12 predicate functions which we use for performance evaluation.
\end{example}
To simulate plausible recommendation criterion we assumed that
a user of the knowledge graph recommendation system is only
interested in ranking the \textsc{PER} entities, therefore the predicate function
only needs to be able to label the \textsc{PER} entities.
\begin{table}[htbp]
  \centering
  \begin{tabular}{r H | c c}
    Relation             & Attribute    & Value 1    & Value 2      \\\hline
    Role                 & role         & author     & director     \\
    EmploymentMembership & employer     & Army       & White House \\
    Resident             & location     & Chinese    & Texas        \\
    Leadership           & subject\_org & Democratic & Parliament   \\
    Origin               & origin       & American   & Russia       \\
    StudentAlum          & almamater    & Harvard    & Stanford     \\
  \end{tabular}
  \caption{List of Predicate Functions used as proxies of an analyst's interests.
    See \exref{ex:eval} for an illustration of the usage of the \texttt{Role=author}
    predicate function.}
  \label{tab:pred-func}
\end{table}


\section{Recommentation Algorithms for the Adept KB}
\label{sec:er-algorithms}
Before we proceed to the discussion and comparison of different vertex
nomination algorithms, let us first understand the implications of the sparsity
and fragmented nature of the Adept KB that we established in
\Secref{sec:graph-sparsity}.  In \Secref{sec:graph-sparsity} we showed that
$66\%$ of the vertices were singleton, the rest of the graph had one giant
component and few small components.  \Figref{fig:adept-cartoon} is a cartoon
representation of a graph with such characteristics.  We can see that out of 20
vertices only a small component can be used as input to a vertex nomination
algorithm and 14 out of 20 vertices are unusable. In this section we discard
such singleton entities and report the result of performing vertex nomination on
the connected portion of the data.
\begin{figure}[htbp]
  \centering
  \includegraphics[trim=0 115 0 0,clip,width=0.4\linewidth,angle=90]{%
    tikz-graph-crop.pdf}
  \caption{Illustration of the relative sparsity in the \texttt{BBN2} Knowledge Graph, with the number of connected and unconnected vertices displayed in similar proportion to the full \texttt{BBN2} KB.}
  \label{fig:adept-cartoon}
\end{figure}
With the above caveat, let us now describe the experimental setup
that remains common across the application of all of the following
algorithms. Let the set of named entities in
the Adept KB that have the type \textsc{PER} be $\mathcal{P}$.  Let us consider
the first row of \tabref{tab:pred-func}.  It can define two predicate function /
criteria for relevance over the elements of $\mathcal{P}$.  The first predicate
selects $\mathcal{Q_1} \subset \mathcal{P}$ such that all elements of
$\mathcal{Q_1}$ have an attribute called \texttt{role} with the value of
\texttt{author}.  Analogously, the second predicate selects $\mathcal{Q}_2$ such
that all elements of $\mathcal{Q}_2$ have \texttt{role=director}.

\Tabref{tab:pred-func} specifies a total of 12 such predicate functions and we
evaluate the performance of a VN algorithm over all of the 12 predicate
functions. For each predicate function we perform 5 trials and measure the
performance in terms of the Area Under Precision-Recall(AUPR) and
Precision@K(P@K).\footnote{We assume that the annotations present in the Adept
KB are complete and correct and use those as the gold standard. This assumption
sometimes turns out to be patently wrong. For example in the case of the
predicate \texttt{Employer=Army} a number of time the Adept KB may have the
annotation that \texttt{Employer=US Army} or \texttt{Employer=Pentagon} but not
the annotation that \texttt{Employer=Army}. In our experiments we side-step
these issues and assume that our task is to faithfully predict the predicate
function as present in the Adept KB.} After performing five trials we average the
performance of the VN algorithm across the trials and calculate the $90\%$
confidence interval assuming the t-distribution for the standard
error(SE). Finally we aggregate the confidence intervals and the average
performance of the algorithms over all the predicate functions and create a
single average performance and performance interval for each algorithm. We
report these metrics in the later sections for different hyper-parameter
settings and most notably for different feature sets.

During each trial for a predicate, we select 10 entities that satisfy the
criterion and which we assume as the positive input that the user provided to
the VN algorithms, similarly we chose 10 \textsc{PER} entities that do not
satisfy the criteria and assume that the user provided those as negative input
to the VN algorithm. During each trial we also remove the edges/attributes that
correspond to the predicate function from the Adept KB before inputting the KB
to the VN algorithm.  We vary the entities used as training data across trials
but ensure that the same entities are used as training input across different
algorithms for a fixed predicate function and trial index. We denote the number
of training instances including both the negative and the positive labeled
instances as $N$ and the remaining instances as $M$.  We denote the set of
\textsc{PER} entities whose relevance is observed with the symbol
$\mathcal{O}$.
The subset of $\mathcal{O}$ that satisfies the predicate function is called
$\mathcal{O}^{+}$ and we define $\mathcal{O}^{-} = \mathcal{O} \setminus \mathcal{O}^{+}$.
Clearly $\vert O \vert = N=20$ for all trials.  Note that $M$ can
hypothetically have zero entities that satisfy the predicate function, however
in practice since there are more than 10 labeled instances this does not happen
for any of the predicates.

\subsection{Entity Recommendation Through the Naive Bayes Model}
\label{sec:er-nb}
Let $v \in \mathcal{P}$. We can represent $v$ through a
feature vector by enumerating all edges that are incident on $v$ and
representing the type of edge along with the neighboring vertex  as the
occurrence of a binary feature. For example, let $\bigstaten{v} \shortarc{r} \bigstaten{q}$
be an arc of type $r$ connecting $v$ to vertex  $q$.
This arc can be represented as the occurrence of the binary feature $\shortarc{r} \bigstaten{q}$.
All such binary features formed by all the arcs that are incident on $v$ can be used to create
a feature vector $f$ that represents $v$.
Given this construction of a feature representation of $v$ we can represent $\mathcal{O}$
as the set $\{(f^i, y^i) | i \in \{1, \ldots, N\}\}$.
Here $y^i\in \mathcal{Y} = \{-1, 1\}$ is the label assigned to the $v^i$ and $f^i$ is its binary
feature representation. $y^i = 1$ means that the feature $f^i$ represents a vertex of interest and $y^i = -1$ means the opposite.
Let $K$ be the length of $f^i$.
We denote the $k$th element of $f^i$ as $f^i[k]$
and we drop the superscript $i$ when the exact data instance is immaterial.
Let $\hat{y}_{\mathrm{NB}}$ denote the estimate produced by the Naive Bayes model.
The Naive Bayes models model the probability of $(x, y)$ and it estimates $\hat{y}_{\mathrm{NB}}$ as follows:
\begin{align}
  p(x, y) = p(f, y) = p(y) p(f \given y) = p(y) \prod_{k=1}^K p(f[k] \given y)\\
  \hat{y}_{\mathrm{NB}} = \argmax{y \in \mathcal{Y}}\ p(y | f) = \argmax{y \in \mathcal{Y}}\ p(y) p(f | y)
\end{align}
The Naive Bayes model can be trained by optimizing the log probability of the entire corpus, $\{(f^i, y^i) | i \in \{1, \ldots, N\}\}$ as specified by the following expression:
\begin{align}
  \sum_{i=1}^N \log(p(v^i, f^i, y^i)) = \sum_{i=1}^N \log(p(y^i)) + \sum_{i=1}^N \log(p(v^i, f^i | y^i))
\end{align}
Once the Naive Bayes model is trained on $\mathcal{O}$ then we can perform vertex nomination
by ranking the an unlabeled instance $v$ according to the posterior probability $p(y=1 | f)$.

\paragraph{Feature Design:}\label{sec:nb-feature-design}
Let us describe the features that we use for the Naive Bayes model with an example.
Consider a an entity named ``Shaikh Ahmed'' that is known to have three properties:
1) It has a Role of ``Gen.'';
2) It has a second Role of ``chief''; and
3) The name of its Residence is ``Afghanistan''.
We begin by concatenating the relation type with the attribute value to get 3 distinct feature types
``Role=Gen.'', ``Role=chief'' and ``Residence=Afghanistan''.
We call these features the ``Concatenative'' features.
Note that this featurization does not share parameters between two different feature even though they may have the same attribute value. For example, it seems plausible that sharing the parameters between ``Residence=Baltimore'' and ``Origin=Baltimore'' could be useful.
We can alleviate this feature sparsity through the addition of ``Backoff'' features
that are based only on the attribute value and not the relation type.
For example we can add the following three features
to ``Shaikh Ahmed''. i) ``Gen.'' ii) ``chief'' and iii) ``Afghanistan''.
We call this augmented featurization ``Concatenation w/ Backoff''.
We call the feature set that  contains only features like i, ii and iii the ``Only Backoff'' feature set.
Finally when we add the documents that a particular entity was mentioned in as features to
``Concatenative'' features then we get the ``Concatenative w/ Doc'' feature set.
For all of the above methods we prune the featureset used to only those that features $f$ that occurred in $\mathcal{O}^{+}$.
separately for each trial.

% Furthermore this featurization is a first order featurization which fails to take into
% account the effect of the conjunction or disjunction of two features.
% Since the distribution of a naive bayes model can change with the repetition of features therefore
% disjunctive features that act as narrow backoff features can be useful for improving the performance
% of the naive bayes model. We report results with pairwise disjunctions and pairwise conjunctions which are
% two possible ways of doing pairwise feature combinations.

\paragraph{Results} We present the aggregate results of the naive bayes classifier's performance in \tabref{tab:perf-nb}.
We can see that the performance of the Naive Bayes method varies a lot with the feature set
and the ``Concatenative'' and ``Concatenative w/ Doc'' feature sets perform the best.
Using backoff features seems to hurt the performance and
the variance in the values of AUPR and P@10 indicates that the method is very sensitive to
the examples that are chosen as inputs. This result is expected since the method only receives 10 data points as input.
However even with 10 data points the Naive Bayes method is able to achieve $30\%$ precision amongst its top 10
recommendations on average which indicates that there are correlations between the values of different attributes
that can be exploited for performing recommendations.

\begin{table}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
    \begin{tabular}{l | c c c c c}
     & Only Backoff      & Concatenative & Concatenative w/ Doc & Concatenative w/ Backoff & Random \\\hline
AUPR & $ 8.8 \pm 3.9 $   & $ 12.6 \pm 5.9 $      & $ 12.5 \pm 5.8 $     & $ 10.9 \pm 5.0 $  & $ 0.9 \pm 0.1 $\\
P@10 & $ 20.4 \pm 13.0 $ & $ 29.6 \pm 16.7 $     & $ 29.8 \pm 18.1 $    & $ 25.3 \pm 12.7 $ & $ 0.2 \pm 0.5 $\\
  \end{tabular}}
  \caption{Performance(\%) with NB with 90\% confidence intervals.}
  \label{tab:perf-nb}
\end{table}

\paragraph{Error Analysis}
We analyzed the coefficients of the trained Naive Bayes models and the predictions made by the system, specifically we tried to understood the reason why the Naive Bayes model sometimes gave a very low rank to relevant entities.
For the first part of the analysis we checked the most important features selected by the naive bayes method for
a few trials of the algorithm for the predicate ``EmploymentMembership=Army''.
Our analysis showed that the NB model gave high score to very sensible feature
values that correlated with the true predicate, for example it gave high weight to the ``EmploymentMembership=Pentagon''
feature when trained to recommend entities that satisfied ``EmploymentMembership=Army''.
Such an assignment of high scores to related entities is an example of the kind of
rules that we wish to learn. Clearly such rules can only be applied to those
\textsc{PER} vertices that have at least two edges. We noticed that the Naive Bayes model was able to learn useful features for recommending entities according to all the relations which we tabulate in \tabref{tab:nb-analysis}.
\begin{table}[htbp]
  \begin{adjustwidth}{-2cm}{}
    \resizebox{1.3\textwidth}{!}{
  \begin{tabular}{l l | l l}
    \textbf{Relation}    & \textbf{Value} & \multicolumn{2}{c}{\textbf{Features}}                             \\\hline
    Role                 & author         & Role=Director                      & Role=Executive               \\
    Role                 & director       & EmploymentMembership=U.S.          & StudentAlum-HQ=U.S.          \\
    EmploymentMembership & Army           & EmploymentMembership=Pentagon      & Role=spokesman               \\
    EmploymentMembership & White House    & EmploymentMembership=United States & Role=representative          \\
    Resident             & Chinese        & Leadership-HQ=Chinese              & Leadership=Communist Party   \\
    Resident             & Texas          & EmploymentMembership=Texas         & Origin=American              \\
    Leadership           & Democratic     & EmploymentMembership=Democratic    & Leadership=Democrats         \\
    Leadership           & Parliament     & Role=representative                & Role=president               \\
    Origin               & American       & Resident=American                  & Role=president               \\
    Origin               & Russia         & Resident=Russian                   & EmploymentMembership=Russian \\
    StudentAlum          & Harvard        & StudentAlum=Harvard Law School     & EmploymentMembership=Harvard \\
    StudentAlum          & Stanford       & Resident=California                & Origin=American              \\
  \end{tabular}}
  \caption{Highly weighted features for recommending \textsc{PER} entities according to relationship criterion.}
  \end{adjustwidth}
  \label{tab:nb-analysis}
\end{table}

Unfortunately, even though the NB algorithm made many correct
recommendations in the top 10 for this criteria those recommendations were
counted as incorrect since they were not part of the Adept KB. This highlights a
challenge associated with evaluating such recommendation tasks where the entire
truth is not known to us even at test time.

For the second part of the analysis we checked why the Naive Bayes model did
not assign a high rank
to the true answers and for half of the cases we found that the \textsc{PER}
entities that were misclassified did not have any feature that occurred in $\mathcal{O}^{+}$.
These issues indicate that there is a natural ceiling to the results that one can obtain without
cleaning it or otherwise modeling the fact that the features may have a information in the form of lexical overlap.

\subsection{Entity Recommendation Through Modified Adsorption}
\label{sec:er-lp}
The MAD algorithm was introduced by \citep{talukdar2009new} as an algorithm
for graph-based semi-supervised learning. The MAD algorithm receives a graph along with
a labeled set $\mathcal{O}$ of entities in the graph and binary labels indicating
their relevance or irrelevance to the predicate as input. As output, the MAD algorithm
produces a soft assignment from the unlabeled nodes to the binary label set.\footnote{The MAD algorithm can be stated in more general terms but the above treatment suffices for our purposes. We refer the reader to~\citep{talukdar2009new} for details of the MAD algorithm
and its analysis.}

The MAD algorithm produces a soft assignment by solving an unconstrained
optimization problem over the space of all labelings where the objective
is defined such that the labeling it produces are ``smooth'', \ie the
labels of nodes that share an edge tend to be similar, and it tends to agree with
the input partial labeling and that it should labeling should be close to an
eigenvector of the graph laplacian of the input graph.
Ideally, the soft assignment that MAD produces should match the true
unknown labels of the unlabeled vertices.

In order to apply MAD which is only defined for graphs with untyped edges to the
Adept KB where the edges have types we use the following trick:
for each edge of the form $\bigstaten{v} \shortarc{r} \bigstaten{q}$ where $v$
is a \textsc{PER} entity, we change the edge from its original form to
$\bigstaten{v} \shortarc{} \bigstaten{rq}$. \Ie for each edge incident on a
\textsc{PER} entity we fuse the edge type with the neighboring vertex and leave
the \textsc{PER} entity as is. This creates many new neighboring vertices and
removes typing information from the edges.
As explained in \secref{sec:er-nb} there are two variants of the Adept KB that
we consider, the first variant does not contain document co-occurrence
information but the second variant contains it.
In order to add document co-occurrence information to the Adept KB
graph we create ``document'' nodes and add edges between \textsc{PER} entities
and the documents that they occurred in. For both of the variants we created
untyped versions of the original graph. We present the results of applying the
MAD algorithm to the Adept KB in \tabref{tab:mad-perf}.

\paragraph{Results} The results indicate that adding the document co-occurrence
information hurts the performance of the MAD algorithm although the decrease is
not statistically significant for either of the metrics. The overall performance of the MAD
algorithms is much worse than the performance of the NB model especially on the
P@10 metric.
% --------------------------------------- %
% MAD Performance with Unlimited Test Set %
% --------------------------------------- %
\begin{table}[htbp]
  \centering
  %\resizebox{\textwidth}{!}
  {
  \begin{tabular}{l | c c H}
         & Concatenative    & Concatenative w/ doc & random          \\\hline
    AUPR & $ 7.0 \pm 2.8 $  & $ 5.4 \pm 1.8 $      & $ 0.9 \pm 0.0 $ \\
    P@10 & $ 9.8 \pm 11.2 $ & $ 9.6 \pm 7.5 $      & $ 0.0 \pm 0.0 $ \\
\end{tabular}}
  \caption{Performance(\%) with MAD with 90\% confidence intervals.}
  \label{tab:mad-perf}
\end{table}

\subsection{Entity Recommendation with Unweighted Random Walks}
\label{sec:er-rw}
The Unweighted Random Walk method uses statistics about the commute
time of unweighted random walks to perform vertex nomination as follows:
Given $\mathcal{O}^{+}$, for each $v \in \mathcal{O}^{+}$ we perform $w$ random walks
of some fixed length $l$ that start from $v$. $w$ and $l$ are hyper-parameters which
we set to $10$ and $3$ after some initial tuning. During each random walk we keep
track of the vertices visited by that random walk and we aggregate the counts of all
the vertices visited by a random walk, over all the random walks performed,
for the set $\mathcal{O}^{+}$ corresponding to a given predicate and trial.
This vector of counts is used to arrange the unlabeled vertices
by assigning a high ranking to those vertices that were visited more often.
We use this method on the two variants of graphs, ``Concatenative'' and ``Concatenative w/ Doc''
which have the same meaning as before.

\paragraph{Results} We present the results in \tabref{tab:perf-rw}.
The results indicate that unweighted random walks perform
worse than the MAD algorithm on the AUPR metric, but they outperform MAD in terms of precision.
The performance of Unweighted Random Walks is worse than the Naive Bayes models.
\begin{table}[htbp]
  \centering
  \begin{tabular}{l | c c}
         & Concatenative & Concatenative w/ Doc \\\hline
    AUPR & $ 2.6 \pm 1.8 $       & $ 1.6 \pm 0.7 $      \\
    P@10 & $ 12.0 \pm 9.0 $      & $ 7.6 \pm 7.3 $      \\
  \end{tabular}
  \caption{Performance(\%) with Random Walks with 90\% confidence intervals.}
  \label{tab:perf-rw}
\end{table}

% \subsection{Entity Recommendation with RESCAL based KBC}
% \label{sec:er-rescal}

\section{Conclusions}
\label{sec:conclusions}
Our experiments with vertex nomination on the Adept KB indicate that
the the closed ontology under which the KB was generated was too restrictive.
Because of the small number of relations in the ontology, a
large number of named entities that were extracted from the text corpus
remained disconnected from other entities, and clearly it is impossible to
use such disconnected entities in a vertex nomination algorithm.

Amongst the small subset of the graph that was not disconnected, it is possible
to perform vertex nomination using the Naive Bayes model however the performance
was low. It is important to note that the Adept ``Knowledge Graph'' behaved more
like a labeled set of features, or a bipartite graph, instead of a communication
graph which the original vertex nomination methods were designed for and applied
to. {The original thought behind applying Vertex Nomination methods to Knowledge
  Graphs was the assumption that the Knowledge Graph is rich in entity to entity
  relations, much like in a social network. What
we observe is that the KG is not rich in Entity to Entity relationships, \eg out
of a total of $134,581$ relations only $3,508$ relations connect one person
entity to another.}

\paragraph{Proposed Next Steps}\label{sec:proposed}
The results indicated that the current Adept KB does not support methods
for vertex nomination since a majority of the entities in the KB are
disconnected, therefore, we propose to investigate
methods to ``expand'' the ontology and introduce more relations in the ontology.


\FW{Learn from the SVM's performance to guide your search amongst models.
  The goal is not to try all of them. The goal is to wisely choose
  amongst the various model based on the performance of existing methods.
  Use Naive Bayes, thresholded NB and other variants of models.\\
  Logistic Regression - L2 regularized linear classifier with log-loss.
  With or Without thresholded features. Thresholded labels.\\
  Linear SVM - L2 regularized linear classifier with hinge loss.
  With or Without thresholded features. Thresholded labels.\\
  Ranking SVM - Utilizing the confidence values as gold standard
  for ranking the known \textit{EOI}.
  With or Without thresholded features. Dont threshold labels.\\
  Linear Regression - L1 - L2 regularized on the confidence values.
  Without thresholding features. Without thresholded labels.\\
  Treat the confidence scores as probabilities, or other bounded real
  values features.\\
  Threshold the confidence score to convert them to binary features.}
\FW{levarage unsupervised label co-occurrence data such as certain
  labels tend to occur with each other.}
\FW{Should all attributes form the document that contained
  the training relations that were used as the criteria of relevance be removed?}
\FW{Use a information gain metric for doing feature selection.}
\FW{Add the performance of MAD when we remove edge attributes all together. (through a union)
  Add the performance of unbiased RW with and without edge attributes.
  Add the performance of RESCAL with and without edge attributes.
  Add the performance of matrix factorization.}
\FW{Remember that for all these models, there's a way to model them
  more compositionally by decomposing the model over the edge types and the
  vertices. Either we can interpret the relations to be different types of
  edges, or equivalently as edge attributes, or we can interpret the combination
  of an edge type and edge value to be different vertices.

  For example let, $\bigstaten{$i$,s}$,
  $\bigstaten{$a$} \shortarc{r} \bigstaten{$b$}$
  be an arc of type $r$ going from vertex $\bigstaten{$a$}$
  to $\bigstaten{$b$}$. We can model the arc non-compositionally
  and consider the $\shortarc{r} \bigstaten{$b$}$ to be a single vertex
  or we can model the graph compositionally and
  consider the arc type to be a feature of the edge.}

% \appendix
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
\remove{
\subsection{Entity Recommendation using binary SVM}
\label{sec:er-feature}

Recall the discussion about \tabref{tab:relation} in \secref{sec:data}.
We showed that a large number of relations in the KB could be considered to
be features of \textsc{PER} entities instead of relations.

This suggests the following simple model for performing entity recommendations:
Represent the labeled examples
as a bag of features and train a linear SVM based classifier to discriminate
between the positive and negative training examples. Subsequently use the
signed distance from the linear classifier's decision hyperplane as a way to
rank the entities in the test set.


The following table shows the performance of the methods on the 12 types of relations,
when the method are given 10 training examples for 5 runs:
% ----------------------------------------------------------- %
% Clipped Linear Classifier Performance on Unlimited Test Set %
% ----------------------------------------------------------- %
\begin{table}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l l l | c c c c}
Relation                        & Attribute    & Value            & \multicolumn{4}{c}{AUPR (Average-Precision)} \\\hline
                                &              &                  & \multicolumn{2}{ c }{w/o doc} & w/ doc             & random \\
                                &              &                  &    w/o clipping    & w/ clipping       & &\\
EmploymentMembership & employer     & Army             & $0.075 \pm 0.055 $ &$0.077 \pm 0.055 $ & $0.066 \pm 0.043 $ & $0.002 \pm 0.000 $  \\
EmploymentMembership & employer     & White\_House     & $0.041 \pm 0.019 $ &$0.044 \pm 0.020 $ & $0.038 \pm 0.017 $ & $0.004 \pm 0.001 $  \\
Leadership           & subject\_org & Democratic       & $0.172 \pm 0.111 $ &$0.171 \pm 0.111 $ & $0.095 \pm 0.122 $ & $0.000 \pm 0.000 $  \\
Leadership           & subject\_org & Parliament       & $0.124 \pm 0.088 $ &$0.124 \pm 0.087 $ & $0.070 \pm 0.037 $ & $0.000 \pm 0.000 $  \\
Origin               & origin       & American         & $0.070 \pm 0.042 $ &$0.073 \pm 0.041 $ & $0.082 \pm 0.031 $ & $0.014 \pm 0.000 $  \\
Origin               & origin       & Russia           & $0.093 \pm 0.071 $ &$0.091 \pm 0.067 $ & $0.082 \pm 0.065 $ & $0.001 \pm 0.000 $  \\
Resident             & location     & Chinese          & $0.063 \pm 0.029 $ &$0.066 \pm 0.032 $ & $0.079 \pm 0.042 $ & $0.004 \pm 0.000 $  \\
Resident             & location     & Texas            & $0.033 \pm 0.014 $ &$0.036 \pm 0.018 $ & $0.027 \pm 0.018 $ & $0.003 \pm 0.000 $  \\
Role                 & role         & author           & $0.025 \pm 0.007 $ &$0.025 \pm 0.007 $ & $0.024 \pm 0.005 $ & $0.021 \pm 0.000 $  \\
Role                 & role         & director         & $0.054 \pm 0.001 $ &$0.054 \pm 0.002 $ & $0.054 \pm 0.001 $ & $0.052 \pm 0.001 $  \\
StudentAlum          & almamater    & Harvard          & $0.271 \pm 0.290 $ &$0.277 \pm 0.301 $ & $0.100 \pm 0.077 $ & $0.001 \pm 0.000 $  \\
StudentAlum          & almamater    & Stanford         & $0.052 \pm 0.073 $ &$0.052 \pm 0.074 $ & $0.041 \pm 0.040 $ & $0.000 \pm 0.000 $  \\
\end{tabular}}
  \caption{Performance with a feature based ranker. The intervals are 90\% confidence intervals.}
  \label{tab:perf-feat}
\end{table}
}
\remove{
\subsection{VN with Semi-Supervised Information}
\label{sec:vn-with-semi}
Semi supervised should only be done, when you think that the
signal really is strong enough. The techniques require data.
}

\remove{
\subsection{Entity Recommendation on Edge attributed Bipartite Graphs under the SBM model}
\label{sec:er-vn}
We want to model the fact that edges have types, and we want to model the fact that
the graph is mostly bipartite. So we have edge attributes, on a bipartite graph.
The natural extension of the SBM model to this situation actually translates
into a generalization of the naive bayes model.

Consider a generalization of the NB model where the components of $f$ belong to
one of $\tilde{K}$ classes. Let $\mathcal{\tilde{K}}$ be the set of classes into
which the entire feature vector can be seprated. Assume that the $i$th component
of $f$, \ie $f[k]$ belong to some class $c_{f[k]}$ and that the probability
$p(x, f[k] | y) = p(c_{f[k]} | y)$. Then $p(x, f, y) = p(y) \prod_{k=1}^K p(x,
f[k] | y) = p(y) \prod_{k=1}^K p(c_{f[k]} | y)$

Let $\Theta \in [0, 1]^{|\mathcal{Y}| \times \mathcal{\tilde{K}}}$ be a matrix
consisting of the parameters $p(c | y)$, \ie $\Theta = [\theta_{lj}]$ where
each $\theta_{lj} = p(c=j | y=l)$. Let $\pi$ be the map that assigns a class to
its feature, \ie $\pi(k) = c_{f[k]}$. Once we know
$\pi, \Theta, \{p(y) | y \in \mathcal{Y}\}$, \ie the
feature class assignments, the conditional probabilities and the prior class
probabilities  then inference can be done through the familiar rule:

\begin{align}
  \hat{y}_{\mathrm{GNB}} = \argmax{}\ p(y) \prod_{k=1}^K \theta_{y, \pi[k]}^{f[k]}
  (1 - \theta_{y, \pi[k]})^{1-f[k]}
\end{align}

The training is again done through the maximization of log-likelihood of
the corpus, however as oppossed to the simple optimization for the case of the
naive-bayes model the optimization now needs to be done over the $K$ global
latent variables that assign a feature to its class. Each latent variable
$\pi[k]$ can take one out of ${\tilde{K}}$ classes, so the total number
of possible combinations if $\tilde{K}^K$. We propose one simple alternating
maximization scheme.

Assume that the continous parameters $\Theta, \{p(y) | y \in \mathcal{Y}$ are
known and that each joint assignment to the latent variables is equally likely.
Then
\begin{align}
  \pi_{\mathrm{GNB}} &= \argmax{\pi \in \Pi} \sum_{i=1}^N \left[ \log(p(y^{(i)})) +
  \sum_{k=1}^K f^{(i)}[k] \log(\theta_{y^{(i)}, \pi[k]})
  + (1 - f^{(i)}[k]) \log(1- \theta_{y^{(i)}, \pi[k]}) \right]\\
  &= \argmax{\pi \in \Pi} \sum_{k=1}^K \left[ \sum_{i=1}^N f^{(i)}[k] \log(\theta_{y^{(i)}, \pi[k]})
  + (1 - f^{(i)}[k]) \log(1- \theta_{y^{(i)}, \pi[k]}) \right]\\
\implies \pi_{\mathrm{GNB}}(k) &= \argmax{\tilde{k} \in [\tilde{K}]}
                                 \sum_{i=1}^N f^{(i)}[k] \log(\theta_{y^{(i)}, \tilde{k}})
  + (1 - f^{(i)}[k]) \log(1- \theta_{y^{(i)}, \tilde{k}}) \\
\end{align}

We note that the assignment of a value to $\pi[k]$ does not affect the
assignment to $\pi[k]$ so we can maximize the objective by decomposing
the objective for each of the latent variables and separately optimizing
each of the sub0objectives.

Once we estimate the global latent variables $\pi[k]$ then it is easy to find
out the prior probabilities $p(y^{(i)})$ and this procedure can be repeated to
convergence. Note that this method induces a natural clustering of the features.


The results are as follows

\begin{table}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l l l | c c c }
Relation                        & Attribute    & Value           & \multicolumn{3}{c}{AUPR (Average-Precision)} \\\hline
                                &              &                 & Only backoff & Only Conjunctive & Random \\
EmploymentMembership & employer     & Army            & $0.029 \pm 0.022 $ & $0.025 \pm 0.026 $ & $0.002 \pm 0.000 $\\
EmploymentMembership & employer     & White\_House    & $0.042 \pm 0.023 $ & $0.041 \pm 0.015 $ & $0.004 \pm 0.000 $\\
Leadership           & subject\_org & Democratic      & $0.204 \pm 0.055 $ & $0.164 \pm 0.041 $ & $0.000 \pm 0.000 $\\
Leadership           & subject\_org & Parliament      & $0.116 \pm 0.077 $ & $0.133 \pm 0.083 $ & $0.000 \pm 0.000 $\\
Origin               & origin       & American        & $0.045 \pm 0.008 $ & $0.048 \pm 0.015 $ & $0.014 \pm 0.001 $\\
Origin               & origin       & Russia          & $0.086 \pm 0.045 $ & $0.063 \pm 0.030 $ & $0.001 \pm 0.000 $\\
Resident             & location     & Chinese         & $0.014 \pm 0.011 $ & $0.049 \pm 0.042 $ & $0.004 \pm 0.000 $\\
Resident             & location     & Texas           & $0.016 \pm 0.012 $ & $0.033 \pm 0.023 $ & $0.003 \pm 0.000 $\\
Role                 & role         & author          & $0.025 \pm 0.006 $ & $0.028 \pm 0.002 $ & $0.021 \pm 0.001 $\\
Role                 & role         & director        & $0.051 \pm 0.003 $ & $0.058 \pm 0.011 $ & $0.052 \pm 0.001 $\\
StudentAlum          & almamater    & Harvard         & $0.089 \pm 0.112 $ & $0.101 \pm 0.033 $ & $0.001 \pm 0.000 $\\
StudentAlum          & almamater    & Stanford        & $0.045 \pm 0.037 $ & $0.094 \pm 0.098 $ & $0.000 \pm 0.000 $\\
\end{tabular}}
  \caption{Performance with Block NB. The intervals are 90\% confidence intervals.}
  \label{tab:perf-block-nb}
\end{table}
}

%% Local Variables:
%% eval: (progn (flycheck-mode -1) (company-mode -1) (orgtbl-mode -1))
%% End:
