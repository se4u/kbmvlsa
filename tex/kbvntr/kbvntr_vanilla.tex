\documentclass[paper=a4,fontsize=11pt]{scrartcl}
\usepackage{underscore,changepage,booktabs}
\usepackage[dvipsnames]{xcolor}
\usepackage[normalem]{ulem}
\usepackage{framed}
\definecolor{shadecolor}{rgb}{1.0,0.8,0.3}
\usepackage[T1]{fontenc}
\usepackage[english]{babel} % English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{amsmath,amsfonts,amsthm,url,xspace,amssymb,mathrsfs}
\usepackage[pdftex]{graphicx}

% http://tex.stackexchange.com/questions/128033
% undefining-leftbar-environment-to-eliminate-multiple-definitions
% Work by adding these two lines.
\let\leftbar\undefined
\let\endleftbar\undefined

\usepackage{thmtools}
\declaretheorem[%
  style=plain,%
  thmbox={style=M,bodystyle=\normalfont},%
  name=Example,%
  within=section,%
  shaded={bgcolor=gray!10},%
]{example}
%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering\normalfont\scshape}
%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\usepackage[disable]{todonotes} % or [disable] notes.
\pagestyle{fancyplain}
\fancyhead{}                        % No page header
\fancyfoot[L]{}                     % Empty
\fancyfoot[C]{}                     % Empty
\fancyfoot[R]{\thepage}             % Pagenumbering
\renewcommand{\headrulewidth}{0pt}  % Remove header underlines
\renewcommand{\footrulewidth}{0pt}  % Remove footer underlines
\setlength{\headheight}{13.6pt}
%%% Define a custome column type.
\usepackage{dcolumn}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\newcommand{\eg}{e.g.,\xspace}
\newcommand{\Eg}{E.g.,\xspace}
\newcommand{\etal}{\textit{et~al.\xspace}}
\newcommand{\etc}{etc.\@\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\Ie}{I.e.,\xspace}
\newcommand{\secref}[1]{section~\ref{#1}}
\newcommand{\Secref}[1]{Section~\ref{#1}}
\newcommand{\tabref}[1]{table~\ref{#1}}
\newcommand{\Tabref}[1]{Table~\ref{#1}}
\newcommand{\exref}[1]{example~\ref{#1}}
\newcommand{\Exref}[1]{Example~\ref{#1}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname*{argmax}}}
\newcommand{\note}[1]{\todo[author=PR,color=blue!40,size=\small,fancyline,inline]{Note: #1}}
\newcommand{\Todo}[1]{\todo[author=PR,size=\small,inline]{Todo: #1}}
\newcommand{\FW}[1]{\todo[author=PR,color=pink!20,size=\small,inline]{Future Work: #1}}


%%% Equation and float numbering
\numberwithin{equation}{section}    % Equationnumbering: section.eq#
\numberwithin{figure}{section}      % Figurenumbering: section.fig#
\numberwithin{table}{section}       % Tablenumbering: section.tab#
%%% Institution and Authors
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}     % Horizontal rule
% \title{
%   % \vspace{-1in}
%   \usefont{OT1}{bch}{b}{n}
%   \normalfont\normalsize\textsc{HLTCOE, Johns Hopkins University}\\[25pt]
%   \horrule{0.5pt}\\[0.4cm]
%   \huge Entity recommendations on a Cold Start Knowledge Graph\\
%   \horrule{2pt}\\[0.5cm]
% }
% \author{
%   \normalfont\normalsize
%   Pushpendre Rastogi\\
%   %[-3pt]\normalsize\today%%Optional Date
% }
\title{
  \vspace{-1in}
  \usefont{OT1}{bch}{b}{n}
  \normalfont\normalsize\textsc{HLTCOE, Johns Hopkins University}\\
  \huge Entity recommendations on a Cold Start Knowledge Graph\\
}
\author{
  \normalfont\normalsize
  Pushpendre Rastogi\\
  [-3pt]\normalsize\today%%Optional Date
}

\date{}

\newcommand{\newrel}{41}
\renewcommand{\cite}[1]{\textcolor{red}{#1}}
\renewcommand{\r}[1]{\textcolor{red}{#1}}
\newcommand{\dataset}[0]{\texttt{BBN2} dataset}
\newcommand{\ontology}{\textsc{BBN Adept Ontology}}
\newcommand{\task}{CS-KBP 2015 task\xspace{}}
\newcommand{\dg}{$^\dagger$}
\newcommand{\ddg}{$^\ddagger$}
\newcommand{\OPTION}[0]{\textcolor{red}{OPTION}}
\newcommand{\PK}{P@K}

%%% Commands for arcs and states.
\def\overstrike#1#2{{\setbox0\hbox{$#2$}\hbox to \wd0{\hss$#1$\hss}\kern-\wd0\box0}}
\newcommand{\bigstaten}[1]{\overstrike{\protect\raisebox{1pt}{\mbox{\scriptsize #1}}}{\mbox{\Large $\bigcirc$}}}
\newcommand{\shortarc}[1]{\protect\raisebox{-1pt}{$\stackrel{#1}{\longrightarrow}$}}
\newcommand\given{\:\vert\:}
\newcommand{\remove}[1]{}
% --------------------------------------------------- %
% Globally Override the special status of # character %
% --------------------------------------------------- %
\catcode`#=12
\begin{document}
\maketitle
\section{Introduction}
\label{sec:introduction}
The predominant way in which analysts interact with and make use of knowledge
graphs is by treating knowledge graphs as databases that support evaluation
of queries. A graph $G$ comprises a vertex set $V$ and a relation
on those vertices $E$. The elements of $E$ are called edges.
The vertices and edges can also have attributes or features.
In the literature for knowledge
graphs vertices may also be called entities or nodes, the edges may be
called relations or predicates or slots and attributes are sometimes called properties or
features. Some literature also assigns types to
vertices and edges which for all purposes are just discrete attributes that may
sometimes be mutually exclusive.

The most common type of queries that are used to interface with
databases, graphical or otherwise, are queries that exist in some logical
algebra. For example a knowledge graph may be stored inside a graph database,
an RDF {triplestore} or even inside XML files and then queried using a graph
query language such as Cypher or Gremlin, SPARQL or XPath. Such interfaces to
knowledge graphs require an analyst to know atleast the following things:
\begin{itemize}
\item The schema of the knowledge graph.
\item An expression from the query language, of the subgraph that the
  analyst wants to select from the knowledge graph.
\end{itemize}
In case the desired subgraph cannot be efficiently expressed in the query
language or if the exact expression is too inefficient to execute, then
alternative queries may be formulated that select an approximations of
the ideal subgraph that we want.

A paradigm for databases queries that is very different from the paradigm
of satisfying descriptive logical programs is the paradigm of
\textit{example based queries} where the analyst wants to retrieve a subset of
vertices and instead of describing that subset through an expression of the query
language she provides {examples} of entities that are elements of
 the desired subset. Sometimes the analyst may also provide negative examples of
 entities that are are not elements of desired subset. The desired subset of
 vertices is also sometimes called the ``{relevant set}'' or
 ``{interesting set}''. Based on the  examples the database guesses the
 analyst's criterion for relevance or interest and {recommends} other
 vertices that would be relevant to the user. Databases that can support such
 example based queries and return a sorted list of relevant entities are called
\textit{Recommendation Systems} or \textit{Recommenders} in short. We call algorithms
that can rank entities in a dataset based on a set of example entities
\textit{Recommendation Algorithms}~(RAs). \Exref{ex:recommender} presents a
concrete example of the utility of Recommenders and the variety of RAs that
can be deployed. Clearly a single RA cannot perform well on all types of graphs and all types of
queries. The performance of an RA would crucially depend on the topology of the
knowledge graph, the complexity of the queries and the expressivity of the
features utilized by the RA.

\begin{example}\label{ex:recommender}
  Consider a social network of sportsmen that contains the height
  and weight of people and the sports that they play as well as their friendship
  status. Assume that an analyst needs to retrieve the adult triathlete
  with the lowest lung capacity from this database.

  Clearly our analyst has her job cut out for her, not only will she have to
  figure out the various ways in which people might express that they are
  triathletes, but the attributes of age and lung capacity don't even exist in
  the dataset. Since triathlete might simply state that they are triathletes or
  instead state the three or more sports that they play individually creating
  the correct filters by hand can be time consuming. On top of that the analyst
  will have to manually figure out some rules for guessing the age and lung capacity of
  a person  based on their height and weight.

  In such a scenario a \textit{Recommender} can be very useful for achieving quick
  results. A general purpose RA based on a probabilistic linear model built with
  third order feature conjunctions could be quite easily figure out the right
  entities to retrieve from the database and give useful hints to the analyst
  for fast prototyping.
\end{example}

To ascertain the extent to which, the most advanced automatically
constructed knowledge graph that has been created under the DARPA DEFT program,
can function as a recommender, we evaluated the performance of a few RAs (See \Tabref{tab:ra}) on the
\dataset (\Secref{sec:data})~\cite{BBN-REPORT} with a few plausible, but
synthetic example based queries (\Secref{sec:evaluation}).
Our results (\Secref{sec:er-algorithms}) suggest that the current state of
the art automatically constructed knowledge graphs are too sparse to fruitfully
function as a recommender. In conclusion (\Secref{sec:conclusions}), we discuss
guidelines to construct ontologies such that the resulting knowledge graph can
function as a recommender.

\begin{table}[htbp]
  \centering
  \begin{tabular}{l l}
    Method              & Type         \\\hline
    Naive Bayes         & Inductive    \\
    %Block Naive Bayes  & Inductive    \\
    %Binary SVM         & Inductive    \\
    Modified Adsorption & Transductive \\
    Random Walk         & Transductive \\
  \end{tabular}
  \caption{List of Recommendation Algorithms}
  \label{tab:ra}
\end{table}

% \begin{table}[htbp]
%   \centering
%   \begin{tabular}{l l l}
%     Transductive & Inductive & Semi Supervised \\
%     Linear & Non Linear \\
%     Generative & Discriminative \\
%     Only positive examples & Both positive and negative examples.
%   \end{tabular}
%   \caption{Criteria for categorization.}
%   \label{tab:cat-crit}
% \end{table}


\section{The \dataset{}}
\label{sec:data}
The TAC Cold Start Knowledge Base Population (CS-KBP) shared task is organized
by NIST to evaluate systems that build a knowledge base (KB) from raw natural
language text~\cite{TAC-KBP-DESC}.  The competing systems extract entities and relations based on
textual clues present in natural language documents and populate a KB according to a shared
schema of entities and relationships. The set of documents is released at the beginning of
the task and the schema is released some time before that.
At the end of the shared task the systems submit their
automatically generated knowledge bases to NIST, which evaluates their accuracy.
BBN participated in the \task where it was the top performing
participant in terms of precision and overall F1 according to the official
results as of November 2015~\cite{BBN-System}. Since BBN's \texttt{BBN2} was one
of its top performing submissions therefore to ascertain the feasibility of using an
automatically constructed knowledge graphs as a recommender we
performed our experiments on \texttt{BBN2}.

The \dataset is a knowledge graph built according to the \ontology.
It contains three standard entity types: \textsc{PER, ORG, LOC}
that refer to Person, Organization and Location respectively as well
as four extra types of \textsc{TITLE, DATE, CRIME, URL} (\Tabref{tab:type}).
We will collectively denote entities
of these types as named entities. Technically we define a named entity to be a
any entity that has only either a canonical string
or a canonical time attached to it and no other attributes.
See~\ref{ex:ne}  for an example.
Because of the errors made during automatic relation extraction there can be multiple
named entities with different identifiers that have approximately the same canonical string
(See \Exref{ex:title-cap}). We call this phenomenon \textit{Denormalization} and we will show
later this phenomenon severely hurts the performance of any recommendation system.
\begin{example}\label{ex:ne}
  The identifier $9dd1d8d3-9a1f-4ec2-a45b-64cce788eb01$ in the dataset has
the type \texttt{Title}
and the canonical string associated with it is \texttt{Almighty King}. This
identifier has no other attributes attached to it.
\end{example}
\begin{example}\label{ex:title-cap}
  The identifiers $e3ac2b1e-cd3b-4cf8-a1fb-b1f71db123df$ and
  $234f3110-c8ca-49aa-ba7b-a45bb5467c38$ have canonical strings \textit{professor} and
  \textit{Professor} respectively which differ only in their capitalization.
\end{example}
As \tabref{tab:type} shows, the \texttt{Person} category is the highest occurring
category with $125$K instantiations
and it is slightly less than double the next largest category of \texttt{Organization}.
The occurrences of the \texttt{URL} type are negligible.
\begin{table}[htbp]
  \centering
  \begin{tabular}{r l l}
    \textbf{Instantiations} & \textbf{Type}& \textbf{Property}\\
    $125022$         & Person             & canonicalString \\
    $69544$          & Organization       & canonicalString \\
    $23754$          & GeoPoliticalEntity & canonicalString \\
    $1761$           & Title              & canonicalString \\
    $819$            & adept-base#Date    & xsdDate         \\
    $333$            & Crime              & canonicalString \\
    $189$            & URL                & canonicalString \\\cline{1-1}
    Total $= 221,547$   \\
  \end{tabular}
  \caption{The number of instantiations(occurrences) of Named Entities of different types.}
  \label{tab:type}
\end{table}
Also we note that the knowledge graph contains a large number of singleton entities.
We can infer this from Tables~\ref{tab:type} and \ref{tab:relation}
since the number of entities is much larger than the total number of edges.

The relations in the \dataset comprise of a small number of relation types.
Just like named entities every instance of a relation has a unique id.
Each instance relates two named entities (See \Exref{ex:rel}).
\begin{example}\label{ex:rel}
  The id $001ae391-3a55-43d1-8d3e-0557bdd943d1$ refers to a relation of the
type \texttt{Resident} which connects its named attribute \texttt{person}
which must have the type \texttt{Person}
with its \texttt{location} attribute which must be of type
\texttt{Geopoliticalentity}.
\end{example}
The total number of regular relations was 41 in \task
but the occurrences for different relations follow a power law
(\Tabref{tab:relation}) and  out of the 20 relations that are observed the
highest occurring \texttt{Role} relation type
that relates a person to his/her title accounts for $30\%$ of the data.
The reason for this skew is that the
natural language text itself has a strong bias about the type of relations that are
reported explicitly and that bias is reflected in the distribution of textual mentions
of relations.
\begin{table}[htbp]
  \resizebox{\textwidth}{!}{
  \begin{tabular}{r l l l}
\textbf{Instances} & \textbf{Relation}       &\textbf{Attribute 1} &\textbf{Attribute 2}\\
42490              & Role                    & person              & role             \\
36631              & EmploymentMembership    & employeeMember      & organization     \\
16288              & Resident                & person              & location         \\
9600               & Leadership              & leader              & affiliatedEntity \\
8705               & Subsidiary\dg           & organization        & subOrganization  \\
7242               & OrgHeadquarter\dg       & organization        & location         \\
2764               & Origin                  & person              & affiliatedEntity \\
1774               & ParentChildRelationship\ddg & parent          & child            \\
1612               & Die                     & person              & place            \\
1317               & SpousalRelationship\ddg & person              & person           \\
1306               & StudentAlum             & studentAlumni       & organization     \\
1281               & Founder                 & founder             & organization     \\
1261               & BeBorn                  & person              & time             \\
530                & InvestorShareholder     & investorShareholder & organization     \\
417                & SiblingRelationship\ddg & person              & person           \\
387                & ChargeIndict            & defendant           & crime            \\
374                & StartOrganization\dg    & organization        & time             \\
369                & Membership\dg           & organization        & member           \\
200                & OrganizationWebsite\dg  & organization        & url              \\
33                 & EndOrganization\dg      & organization        & time             \\\cline{1-1}
Total $= 134,581$ \\
\end{tabular}}
  \caption{Relations Types and their attributes. \dg indicates relations for
    which neither of the arguments are of the type
    \texttt{Person}. \ddg indicates relations for which both of the
    arguments have the type \texttt{Person}.}
  \label{tab:relation}
\end{table}

\subsection{Relations or Attributes}
\label{sec:relat-or-attr}
As shown in \tabref{tab:relation} the \texttt{Role} relation does not relate two people.
It only relates a person and its title.
In this sense, the \texttt{Role} relation acta as a feature or an attribute of a person
instead of a relation between them, since the \texttt{Role} for one person can be completely
independent of the Role of another person unlike the \texttt{Sibling, Spousal and ParentChild}
relationships. The \texttt{Sibling, Spousal and ParentChild} relationships always affects two
people simultaneously in the sense that if a person \textsc{B} is set to be the sibling of \textsc{A} then
\textsc{A} must be set to the sibling of \textsc{B}.
We note that 11 out of 20 relations including the top four most observed relations,
act as features of persons instead of relations. So the \textit{intra-person} relational
information is lower in comparison to \textit{person-feature} information.
Also note that only 6 relations relate organization to other organizations or to locations,
so the \textit{intra-feature} relational information is also quite low.

% \subsection{Data Cleaning}
% \label{sec:data-cleaning}
% \Todo{As~\exref{ex:title-cap} illustrated some of the titles in the \dataset
%   only differ from each other by their capitalization.
%   During the task of vertex nomination matching such fragments can help in reducing sparsity.
%   but data cleaning can be handled more flexibly (though perhaps less efficiently)
%   as a similarity metric between the fragments.}

\section{Synthetic Evaluation}
\label{sec:evaluation}
In order to evaluate RAs we need to simulate the kind of example based queries that an analyst might submit to an RA.
In abscence of actual users minimally we require a synthetic predicate function that can act as a proxy of normal user's interests
and that can label whether an entity is of interest or not. The RA can be trained by providing a small number of labeled examples
generated from the predicate function and tested on the rest of the entities that were not provided as labeled inputs.

We chose to use an existing binarized feature as a synthetic predicate function that
acts as the oracle that determines whether an entity is relevant or not.
During training and testing we remove this feature from the data and
only present a few examples of entities with the values of this feature as labels.
The algorithm then has to predict whether this hidden feature is $0$ or $1$ for the
rest of the people on the basis of the training data and then to assign a rank to them
such that people with feature value $1$ rank higher than people with feature value of $0$.
Two standard evaluation metrics for these tasks are the {Precision@K}(\PK) metric and the
{Area Under Precision Recall Curve}(AUPR) metric and we would report the performance of the systems
on these two metrics and compare them to a random baseline.
\begin{example}\label{ex:eval}
    For example, a user of the recommendation system may mark certain employees of the
  ``White House'' as people of interest and based on these examples the recommendation
  system needs to assign a high rank to the remaining employees of the ``White House'' and
  a low rank to the persons. Note that the recommendation system does not receive the
  feature that the person was the employee of the ``White House'' during operation
  and it has to make a guess on the basis of other persons and their features.
  Consider a second example, where a predicate function may mark people who are
  ``authors'' as people of interest and request the recommendation system to find more
  people who are ``authors''. \tabref{tab:pred-func} lists the 6 Relations types that we
  use to create a 12 predicate functions which we use for performance evaluation.
\end{example}
To simulate plausible recommendation criterion we assumed that
a user of the knowledge graph recommendation system is only
interested in ranking the \textsc{PER} entities, therefore the predicate function
only needs to be able to label the \textsc{PER} entities.
\begin{table}[htbp]
  \centering
  \begin{tabular}{r H | c c}
    Relation             & Attribute    & Value 1    & Value 2      \\\hline
    Role                 & role         & author     & director     \\
    EmploymentMembership & employer     & Army       & White House \\
    Resident             & location     & Chinese    & Texas        \\
    Leadership           & subject\_org & Democratic & Parliament   \\
    Origin               & origin       & American   & Russia       \\
    StudentAlum          & almamater    & Harvard    & Stanford     \\
  \end{tabular}
  \caption{List of Predicate Functions used as proxies of an analyst's interests.
    See \exref{ex:eval} for an illustration of the usage of the \texttt{Role=author}
    predicate function.}
  \label{tab:pred-func}
\end{table}
\FW{levarage unsupervised label co-occurrence data such as certain
  labels tend to occur with each other.}
\FW{Should all features form the document that contained
  the training relations that were used as the criteria of relevance be removed?}

\section{Recommentation Algorithms for Automatically Constructed Knowledge Graphs}
\label{sec:er-algorithms}
Since the knowledge graphs are automatically constructed therefore they are noisy.
We first try simple model to see the performance.

% Let us introduce some notation to explain the task of Entity Recommendation.
% Let $\mathcal{V}$ be a finite set and let
% $\mathcal{O} \subset \mathcal{I} \subset \mathcal{V}$.
% $\mathcal{V}$ denotes the set of all entities, $\mathcal{I}$ denotes the set
% of \textit{entities of interest} and $\mathcal{O}$ denotes those entities for
% which it is known that they are elements of $\mathcal{I}$. The task of entity
% recommendation is to produce a ranking $\pi$ of the vertices in
% $\mathcal{O}^C = \mathcal{V} \setminus \mathcal{O}$
% such that vertices belonging to $\mathcal{T} = \mathcal{I} - \mathcal{O}$ are
% ranked higher than the vertices in $\mathcal{V} - \mathcal{I}$.

\subsection{Entity Recommendation Through the Naive Bayes Model}
\label{sec:er-nb}
Consider a set of observations $\{(f^i, y^i) | i \in \{1, \ldots, N\}\}$.
Here $y^i$ is the label assigned to the $i$th instance and $f^i$ is the
feature representation of $x^i$. Let $f^i$ is a binary vector of size $K$.
Let $y^i \in \{0, 1\}$. We denote the $k_{\mathrm{th}}$ element of $f^i$ as $f^i[k]$
and we drop the superscript $i$ when the exact data instance is immaterial.
Let $\hat{y}_{\mathrm{NB}}$ denote the estimate produced by the naive bayes model.
Then the Naive Bayes models model the probability of $(x, y)$ very simplistically
and they use that probability to estimate $\hat{y}_{\mathrm{NB}}$ as follows:
\begin{align}
  p(x, y) = p(f, y) = p(y) p(x, f \given y) = p(y) \prod_{k=1}^K p(f[k] \given y)\\
  \hat{y}_{\mathrm{NB}} = \argmax{y \in \mathcal{Y}} p(y | x, f) = \argmax{y \in \mathcal{Y}} p(y) p(x, f | y)
\end{align}
The training of naive bayes models is done by
maximizing the log probability of the entire corpus specified by the following expression:
\begin{align}
  \sum_{i=1}^N \log(p(x^i, f^i, y^i)) = \sum_{i=1}^N \log(p(y^i)) + \sum_{i=1}^N \log(p(x^i, f^i | y^i))
\end{align}
We perform vertex nomination by training the naive bayes model
and then ranking the rest of the instances according to their
positive class posterior probability.

\paragraph{Feature Design:}\label{sec:nb-feature-design}
Although the naive bayes model is conceptually very simple and does have many hyper parameters, it
is quite sensitive to the features that are fed to it as input.

Let us explain our featurization of the model using an example:
Consider an \textsc{PER} entity named ``Shaikh Ahmed'' that is known to
have three properties: 1) It has a Role of ``Gen.''
2) It has a second Role of ``chief'' and
3) The name of its Residence is ``Afghanistan''.
We begin by concatenating the relation type with the attribute value to get 3 distinct feature types
``Role-Gen.'', ``Role-chief'' and ``Residence-Afghanistan''. We call these features the ``concatenative'' features.
Note that this featurization can
lead to sparsity since it would assign a different feature to an entity for which we know that its
``Origin'' is ``Afghanistan''. We can alleviate this problem of
feature sparsity through the addition of ``attribute-based'' backoff features
based solely on the attribute values and not the relation types.
For example we can add the following three features
to ``Shaikh Ahmed''. i) ``Gen.'' ii) ``chief'' and iii) ``Afghanistan''.
We call this augmented featurization ``Concat W/ Backoff''.
% Furthermore this featurization is a first order featurization which fails to take into
% account the effect of the conjunction or disjunction of two features.
% Since the distribution of a naive bayes model can change with the repetition of features therefore
% disjunctive features that act as narrow backoff features can be useful for improving the performance
% of the naive bayes model. We report results with pairwise disjunctions and pairwise conjunctions which are
% two possible ways of doing pairwise feature combinations.
For all these methods we prune the actual features
used to only those that actually occurred in the positively
labeled portion of the training set, separately for each trial.

\begin{table}[htbp]
  \centering
  \resizebox{\textwidth}{!}
  {
  \begin{tabular}{l | c c c c c}
     & Only Backoff      & Concatenative w/o Doc & Concatenative w/ Doc & Concat w/ Backoff & Random \\\hline
AUPR & $ 8.8 \pm 3.9 $   & $ 12.6 \pm 5.9 $      & $ 12.5 \pm 5.8 $     & $ 10.9 \pm 5.0 $  & $ 1.0 \pm 0.1 $\\
P@10 & $ 20.4 \pm 13.0 $ & $ 29.6 \pm 16.7 $     & $ 29.8 \pm 18.1 $    & $ 25.3 \pm 12.7 $ & $ 2.0 \pm 4.3 $\\
  \end{tabular}}
  \caption{Performance with NB. The intervals are 90\% confidence intervals.}
  \label{tab:perf-nb}
\end{table}

\remove{
\subsection{Entity Recommendation using binary SVM}
\label{sec:er-feature}

Recall the discussion about \tabref{tab:relation} in \secref{sec:data}.
We showed that a large number of relations in the KB could be considered to
be features of \textsc{PER} entities instead of relations.

This suggests the following simple model for performing entity recommendations:
Represent the labeled examples
as a bag of features and train a linear SVM based classifier to discriminate
between the positive and negative training examples. Subsequently use the
signed distance from the linear classifier's decision hyperplane as a way to
rank the entities in the test set.
\FW{Learn from the SVM's performance to guide your search amongst models.
  The goal is not to try all of them. The goal is to wisely choose
  amongst the various model based on the performance of existing methods.
  Use Naive Bayes, thresholded NB and other variants of models.\\
  Logistic Regression - L2 regularized linear classifier with log-loss.
  With or Without thresholded features. Thresholded labels.\\
  Linear SVM - L2 regularized linear classifier with hinge loss.
  With or Without thresholded features. Thresholded labels.\\
  Ranking SVM - Utilizing the confidence values as gold standard
  for ranking the known \textit{EOI}.
  With or Without thresholded features. Dont threshold labels.\\
  Linear Regression - L1 - L2 regularized on the confidence values.
  Without thresholding features. Without thresholded labels.\\
  Treat the confidence scores as probabilities, or other bounded real
  values features.\\
  Threshold the confidence score to convert them to binary features.}

The following table shows the performance of the methods on the 12 types of relations,
when the method are given 10 training examples for 5 runs:

% ------------------------------------------------------------------ %
% Clipped Linear Classifier Performance on Limited Balanced Test Set %
% ------------------------------------------------------------------ %
% \begin{table}[htbp]
%   \centering
%   \resizebox{\textwidth}{!}{
%   \begin{tabular}{l l l | H H H c c c}
% Relation                        & Attribute    & Value           & \multicolumn{3}{H}{P\@10}     & \multicolumn{3}{c}{AUPR (Average-Precision)} \\\hline
%                                 &              &                 & w/o doc & w/ doc & random     & w/o doc & w/ doc & random \\
% EmploymentMembership & employer     & Army            & $0.940 \pm 0.052$  & $0.940 \pm 0.052$  & $0.560 \pm 0.091$  & $0.822 \pm 0.056$  & $0.853 \pm 0.036$  & $0.544 \pm 0.059$   \\
% EmploymentMembership & employer     & White\_House    & $0.860 \pm 0.128$  & $0.820 \pm 0.104$  & $0.490 \pm 0.100$  & $0.775 \pm 0.073$  & $0.737 \pm 0.080$  & $0.538 \pm 0.033$   \\
% Leadership           & subject\_org & Democratic      & $0.640 \pm 0.052$  & $0.660 \pm 0.052$  & $0.510 \pm 0.043$  & $0.909 \pm 0.060$  & $0.932 \pm 0.023$  & $0.626 \pm 0.071$   \\
% Leadership           & subject\_org & Parliament      & $0.500 \pm 0.000$  & $0.500 \pm 0.000$  & $0.500 \pm 0.000$  & $0.960 \pm 0.052$  & $0.942 \pm 0.082$  & $0.676 \pm 0.114$   \\
% Origin               & origin       & American        & $0.720 \pm 0.265$  & $0.820 \pm 0.141$  & $0.500 \pm 0.067$  & $0.681 \pm 0.162$  & $0.748 \pm 0.082$  & $0.551 \pm 0.043$   \\
% Origin               & origin       & Russia          & $1.000 \pm 0.000$  & $0.980 \pm 0.043$  & $0.450 \pm 0.074$  & $0.963 \pm 0.022$  & $0.919 \pm 0.057$  & $0.530 \pm 0.055$   \\
% Resident             & location     & Chinese         & $0.920 \pm 0.171$  & $0.920 \pm 0.171$  & $0.500 \pm 0.095$  & $0.806 \pm 0.113$  & $0.820 \pm 0.119$  & $0.535 \pm 0.029$   \\
% Resident             & location     & Texas           & $0.780 \pm 0.124$  & $0.760 \pm 0.160$  & $0.440 \pm 0.056$  & $0.691 \pm 0.067$  & $0.661 \pm 0.114$  & $0.499 \pm 0.027$   \\
% Role                 & role         & author          & $0.620 \pm 0.124$  & $0.480 \pm 0.141$  & $0.600 \pm 0.077$  & $0.588 \pm 0.074$  & $0.508 \pm 0.060$  & $0.569 \pm 0.043$   \\
% Role                 & role         & director        & $0.580 \pm 0.171$  & $0.600 \pm 0.178$  & $0.600 \pm 0.109$  & $0.567 \pm 0.090$  & $0.552 \pm 0.067$  & $0.581 \pm 0.050$   \\
% StudentAlum          & almamater    & Harvard         & $0.940 \pm 0.085$  & $0.940 \pm 0.085$  & $0.520 \pm 0.112$  & $0.961 \pm 0.028$  & $0.936 \pm 0.063$  & $0.546 \pm 0.043$   \\
% StudentAlum          & almamater    & Stanford        & $0.500 \pm 0.000$  & $0.500 \pm 0.000$  & $0.500 \pm 0.000$  & $0.993 \pm 0.014$  & $0.985 \pm 0.031$  & $0.697 \pm 0.128$   \\
% \end{tabular}}
%   \caption{Performance with a feature based ranker. The intervals are 90\% confidence intervals.}
%   \label{tab:perf-feat}
% \end{table}

% ----------------------------------------------------------- %
% Clipped Linear Classifier Performance on Unlimited Test Set %
% ----------------------------------------------------------- %
\begin{table}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l l l | c c c c}
Relation                        & Attribute    & Value            & \multicolumn{4}{c}{AUPR (Average-Precision)} \\\hline
                                &              &                  & \multicolumn{2}{ c }{w/o doc} & w/ doc             & random \\
                                &              &                  &    w/o clipping    & w/ clipping       & &\\
EmploymentMembership & employer     & Army             & $0.075 \pm 0.055 $ &$0.077 \pm 0.055 $ & $0.066 \pm 0.043 $ & $0.002 \pm 0.000 $  \\
EmploymentMembership & employer     & White\_House     & $0.041 \pm 0.019 $ &$0.044 \pm 0.020 $ & $0.038 \pm 0.017 $ & $0.004 \pm 0.001 $  \\
Leadership           & subject\_org & Democratic       & $0.172 \pm 0.111 $ &$0.171 \pm 0.111 $ & $0.095 \pm 0.122 $ & $0.000 \pm 0.000 $  \\
Leadership           & subject\_org & Parliament       & $0.124 \pm 0.088 $ &$0.124 \pm 0.087 $ & $0.070 \pm 0.037 $ & $0.000 \pm 0.000 $  \\
Origin               & origin       & American         & $0.070 \pm 0.042 $ &$0.073 \pm 0.041 $ & $0.082 \pm 0.031 $ & $0.014 \pm 0.000 $  \\
Origin               & origin       & Russia           & $0.093 \pm 0.071 $ &$0.091 \pm 0.067 $ & $0.082 \pm 0.065 $ & $0.001 \pm 0.000 $  \\
Resident             & location     & Chinese          & $0.063 \pm 0.029 $ &$0.066 \pm 0.032 $ & $0.079 \pm 0.042 $ & $0.004 \pm 0.000 $  \\
Resident             & location     & Texas            & $0.033 \pm 0.014 $ &$0.036 \pm 0.018 $ & $0.027 \pm 0.018 $ & $0.003 \pm 0.000 $  \\
Role                 & role         & author           & $0.025 \pm 0.007 $ &$0.025 \pm 0.007 $ & $0.024 \pm 0.005 $ & $0.021 \pm 0.000 $  \\
Role                 & role         & director         & $0.054 \pm 0.001 $ &$0.054 \pm 0.002 $ & $0.054 \pm 0.001 $ & $0.052 \pm 0.001 $  \\
StudentAlum          & almamater    & Harvard          & $0.271 \pm 0.290 $ &$0.277 \pm 0.301 $ & $0.100 \pm 0.077 $ & $0.001 \pm 0.000 $  \\
StudentAlum          & almamater    & Stanford         & $0.052 \pm 0.073 $ &$0.052 \pm 0.074 $ & $0.041 \pm 0.040 $ & $0.000 \pm 0.000 $  \\
\end{tabular}}
  \caption{Performance with a feature based ranker. The intervals are 90\% confidence intervals.}
  \label{tab:perf-feat}
\end{table}
}
\subsection{Entity Recommendation Through Label Propagation}
\label{sec:er-lp}
\FW{Remember that for all these models, there's a way to model them
more compositionally by decomposing the model over the edge types and the vertices.}
There are two ways of interpreting the graph.
Either we can interpret the relations to be different types of edges,
or equivalently as edge attributes,
or we can interpret the combination of an
edge type and edge value to be different vertices.

For example let, $\bigstaten{$i$,s}$,
$\bigstaten{$a$} \shortarc{r} \bigstaten{$b$}$
be an arc of type $r$ going from vertex $\bigstaten{$a$}$
to $\bigstaten{$b$}$. We can model the arc non-compositionally
and consider the $\shortarc{r} \bigstaten{$b$}$ to be a single vertex
or we can model the graph compositionally and
consider the arc type to be a feature of the edge.

For sake of simplicity we would first implement the non-compositional
method. The non-compositional method has the ideal property that
there are no longer different types of edges. All edges are of the same
type. The downside is that now there are a much larger number of vertices.
and they can not share parameters.

Using the modified adsorption algorithm we obtain the following result:
% ---------------------------------------------- %
% MAD Performance with Limited Balanced Test Set %
% ---------------------------------------------- %
% \begin{table}[htbp]
%   \centering
%   \resizebox{\textwidth}{!}{
%   \begin{tabular}{l l l | c c c}
% Relation                        & Attribute    & Value           & \multicolumn{3}{c}{AUPR (Average-Precision)} \\\hline
%                                 &              &                 & w/o doc & w/ doc & random \\
% EmploymentMembership & employer     & Army            &  $0.909 \pm 0.046$   &  $0.909 \pm 0.046$   &  $0.539 \pm 0.064$  \\
% EmploymentMembership & employer     & White\_House    &  $0.810 \pm 0.052$   &  $0.810 \pm 0.052$   &  $0.545 \pm 0.055$  \\
% Leadership           & subject\_org & Democratic      &  $0.925 \pm 0.102$   &  $0.925 \pm 0.102$   &  $0.567 \pm 0.116$  \\
% Leadership           & subject\_org & Parliament      &  $0.975 \pm 0.052$   &  $0.975 \pm 0.052$   &  $0.604 \pm 0.101$  \\
% Origin               & origin       & American        &  $0.752 \pm 0.111$   &  $0.752 \pm 0.111$   &  $0.559 \pm 0.083$  \\
% Origin               & origin       & Russia          &  $0.968 \pm 0.040$   &  $0.968 \pm 0.040$   &  $0.558 \pm 0.063$  \\
% Resident             & location     & Chinese         &  $0.931 \pm 0.076$   &  $0.931 \pm 0.076$   &  $0.553 \pm 0.058$  \\
% Resident             & location     & Texas           &  $0.795 \pm 0.065$   &  $0.795 \pm 0.065$   &  $0.491 \pm 0.069$  \\
% Role                 & role         & author          &  $0.540 \pm 0.113$   &  $0.540 \pm 0.113$   &  $0.538 \pm 0.120$  \\
% Role                 & role         & director        &  $0.517 \pm 0.069$   &  $0.517 \pm 0.069$   &  $0.536 \pm 0.066$  \\
% StudentAlum          & almamater    & Harvard         &  $0.986 \pm 0.016$   &  $0.986 \pm 0.016$   &  $0.520 \pm 0.073$  \\
% StudentAlum          & almamater    & Stanford        &  $1.000 \pm 0.000$   &  $1.000 \pm 0.000$   &  $0.628 \pm 0.196$  \\
% \end{tabular}}
%   \caption{Performance with MAD. The intervals are 90\% confidence intervals.}
%   \label{tab:perf-feat}
% \end{table}

% --------------------------------------- %
% MAD Performance with Unlimited Test Set %
% --------------------------------------- %
\begin{table}[htbp]
  \centering
  %\resizebox{\textwidth}{!}
  {
  \begin{tabular}{l  | c c c}
         & w/o doc          & w/ doc           & random          \\
    AUPR & $ 7.0 \pm 2.8 $  & $ 5.4 \pm 1.8 $  & $ 1.2 \pm 0.0 $ \\
    P@10 & $ 9.8 \pm 11.2 $ & $ 12.3 \pm 9.6 $ & $ 0 $           \\
\end{tabular}}
  \caption{Performance with MAD. The intervals are 90\% confidence intervals.}
  \label{tab:perf-label-prop}
\end{table}


Although many other methods can be used on this bipartite graph which has a single edge type
such as other variants of label propagation or variants of matrix factorization. All of those
methods would have the drawback that the vertices would remain non-compositional.
\FW{Add the performance of MAD when we remove edge attributes all together. (through a union)
  Add the performance of unbiased RW with and without edge attributes.
  Add the performance of RESCAL with and without edge attributes.
  Add the performance of matrix factorization.}


\remove{
\subsection{Entity Recommendation on Edge attributed Bipartite Graphs under the SBM model}
\label{sec:er-vn}
We want to model the fact that edges have types, and we want to model the fact that
the graph is mostly bipartite. So we have edge attributes, on a bipartite graph.
The natural extension of the SBM model to this situation actually translates
into a generalization of the naive bayes model.

Consider a generalization of the NB model where the components of $f$ belong to
one of $\tilde{K}$ classes. Let $\mathcal{\tilde{K}}$ be the set of classes into
which the entire feature vector can be seprated. Assume that the $i$th component
of $f$, \ie $f[k]$ belong to some class $c_{f[k]}$ and that the probability
$p(x, f[k] | y = p(c_{f[k]} | y)$. Then $p(x, f, y) = p(y) \prod_{k=1}^K p(x,
f[k] | y) = p(y) \prod_{k=1}^K p(c_{f[k]} | y)$

Let $\Theta \in [0, 1]^{|\mathcal{Y}| \times \mathcal{\tilde{K}}}$ be a matrix
consisting of the parameters $p(c | y)$, \ie $\Theta = [\theta_{lj}]$ where
each $\theta_{lj} = p(c=j | y=l)$. Let $\pi$ be the map that assigns a class to
its feature, \ie $\pi(k) = c_{f[k]}$. Once we know
$\pi, \Theta, \{p(y) | y \in \mathcal{Y}\}$, \ie the
feature class assignments, the conditional probabilities and the prior class
probabilities  then inference can be done through the familiar rule:

\begin{align}
  \hat{y}_{\mathrm{GNB}} = \argmax{} p(y) \prod_{k=1}^K \theta_{y, \pi[k]}^{f[k]}
  (1 - \theta_{y, \pi[k]})^{1-f[k]}
\end{align}

The training is again done through the maximization of log-likelihood of
the corpus, however as oppossed to the simple optimization for the case of the
naive-bayes model the optimization now needs to be done over the $K$ global
latent variables that assign a feature to its class. Each latent variable
$\pi[k]$ can take one out of ${\tilde{K}}$ classes, so the total number
of possible combinations if $\tilde{K}^K$. We propose one simple alternating
maximization scheme.

Assume that the continous parameters $\Theta, \{p(y) | y \in \mathcal{Y}$ are
known and that each joint assignment to the latent variables is equally likely.
Then
\begin{align}
  \pi_{\mathrm{GNB}} &= \argmax{\pi \in \Pi} \sum_{i=1}^N \left[ \log(p(y^{(i)})) +
  \sum_{k=1}^K f^{(i)}[k] \log(\theta_{y^{(i)}, \pi[k]})
  + (1 - f^{(i)}[k]) \log(1- \theta_{y^{(i)}, \pi[k]}) \right]\\
  &= \argmax{\pi \in \Pi} \sum_{k=1}^K \left[ \sum_{i=1}^N f^{(i)}[k] \log(\theta_{y^{(i)}, \pi[k]})
  + (1 - f^{(i)}[k]) \log(1- \theta_{y^{(i)}, \pi[k]}) \right]\\
\implies \pi_{\mathrm{GNB}}(k) &= \argmax{\tilde{k} \in [\tilde{K}]}
                                 \sum_{i=1}^N f^{(i)}[k] \log(\theta_{y^{(i)}, \tilde{k}})
  + (1 - f^{(i)}[k]) \log(1- \theta_{y^{(i)}, \tilde{k}}) \\
\end{align}

We note that the assignment of a value to $\pi[k]$ does not affect the
assignment to $\pi[k]$ so we can maximize the objective by decomposing
the objective for each of the latent variables and separately optimizing
each of the sub0objectives.

Once we estimate the global latent variables $\pi[k]$ then it is easy to find
out the prior probabilities $p(y^{(i)})$ and this procedure can be repeated to
convergence. Note that this method induces a natural clustering of the features.


The results are as follows

\begin{table}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l l l | c c c }
Relation                        & Attribute    & Value           & \multicolumn{3}{c}{AUPR (Average-Precision)} \\\hline
                                &              &                 & Only backoff & Only Conjunctive & Random \\
EmploymentMembership & employer     & Army            & $0.029 \pm 0.022 $ & $0.025 \pm 0.026 $ & $0.002 \pm 0.000 $\\
EmploymentMembership & employer     & White\_House    & $0.042 \pm 0.023 $ & $0.041 \pm 0.015 $ & $0.004 \pm 0.000 $\\
Leadership           & subject\_org & Democratic      & $0.204 \pm 0.055 $ & $0.164 \pm 0.041 $ & $0.000 \pm 0.000 $\\
Leadership           & subject\_org & Parliament      & $0.116 \pm 0.077 $ & $0.133 \pm 0.083 $ & $0.000 \pm 0.000 $\\
Origin               & origin       & American        & $0.045 \pm 0.008 $ & $0.048 \pm 0.015 $ & $0.014 \pm 0.001 $\\
Origin               & origin       & Russia          & $0.086 \pm 0.045 $ & $0.063 \pm 0.030 $ & $0.001 \pm 0.000 $\\
Resident             & location     & Chinese         & $0.014 \pm 0.011 $ & $0.049 \pm 0.042 $ & $0.004 \pm 0.000 $\\
Resident             & location     & Texas           & $0.016 \pm 0.012 $ & $0.033 \pm 0.023 $ & $0.003 \pm 0.000 $\\
Role                 & role         & author          & $0.025 \pm 0.006 $ & $0.028 \pm 0.002 $ & $0.021 \pm 0.001 $\\
Role                 & role         & director        & $0.051 \pm 0.003 $ & $0.058 \pm 0.011 $ & $0.052 \pm 0.001 $\\
StudentAlum          & almamater    & Harvard         & $0.089 \pm 0.112 $ & $0.101 \pm 0.033 $ & $0.001 \pm 0.000 $\\
StudentAlum          & almamater    & Stanford        & $0.045 \pm 0.037 $ & $0.094 \pm 0.098 $ & $0.000 \pm 0.000 $\\
\end{tabular}}
  \caption{Performance with Block NB. The intervals are 90\% confidence intervals.}
  \label{tab:perf-block-nb}
\end{table}
}

\subsection{Entity Recommendation with Random Walks}
\label{sec:er-rw}



\subsection{Entity Recommendation with RESCAL based KBC}
\label{sec:er-rescal}

% \subsection{VN with Semi-Supervised Information}
% \label{sec:vn-with-semi}
% Semi supervised should only be done, when you think that the
% signal really is strong enough. The techniques require data.

\section{Conclusions}
\label{sec:conclusions}

\begin{snugshade}
  {vertex nomination was designed for communication graphs, and thaere has
been interest in extending them to comm nets. the reson for applying VN to KG is
because of the assumption that the KG is rich in entity to entity relations that
have been acquired, much like in a social network. Friends are connected. What
if we observe is that the KG is not rich in E2E relxn . It is rich in doc
cooccurrence. In a social net conn. doc co-oucc is third person comm about 2
entities. The analogy from a social network connection can shift to doc - cooucc
in news stories. the closes we have to a social network is that a third person
is communicating about two entities.}
\end{snugshade}
\bibliographystyle{plain}
\bibliography{references}
\end{document}

%% Local Variables:
%% eval: (progn (flycheck-mode -1) (company-mode -1))
%% End: